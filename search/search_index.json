{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is Daipe? \u00b6 Daipe is an Enterprise AI Platform which helps you to prepare data, build ML models and productionalize them in the enterprise environment fast. Learn how to create a first Daipe project \u2192","title":"Daipe AI Platform Documentation"},{"location":"advanced/","text":"Advanced Daipe features \u00b6 Check our best-practices for setting up Daipe on MS Azure Manage & monitor quality of your data using SettleDQ","title":"Overview"},{"location":"azure-devops-ci-cd/","text":"DevOps CI/CD setup \u00b6 Update project variables \u00b6 Set SERVICE_CONNECTION_NAME variables for your newly created project in .cicd/variables/variables.yml . Commit the changes: set the name of the branch, eq. update-variables check Create pull request It will automatically create Pull request to the master branch, get it approved and merge the changes. The deployment pipeline will be executed automatically, after the merge of updated variables. You can find it running under the Pipelines tab. The project will be deployed to the DEV Databricks and pipeline will ask for permission to the service connection, as it is shown on the picture below.","title":"Setup CI/CD on Azure DevOps"},{"location":"azure-logger-bundle/","text":"Logging into Azure setup \u00b6 The Azure logger bundle package allows you to save logs from your Daipe project into Azure Application Insights which persists your project's logs in a queryable and visualizable way. That allows you to monitor your application's progress and health with ease. Installation \u00b6 poetry add azure-logger-bundle Usage \u00b6 Get the instrumentation key In your project's src/[ROOT_MODULE]/_config/config.yaml parameters : azureloggerbundle : enabled : True app_insights : instrumentation_key : xxxx-xxxx-xxxx-xxxx or use an environment variable instrumentation_key : \"%env(MY_SECRET_ENV)%\" Logs example \u00b6","title":"Logging into Azure setup"},{"location":"azure-setup-overview/","text":"Process Overview \u00b6 The schema above describes the full process of Daipe AI Platform setup: Setting up Azure Environment Spinning up Data Lake Resources Creating Daipe project from template","title":"Setup overview"},{"location":"azure-setup/","text":"Setting up Azure Environment \u00b6 Go to setup steps \u2193 Prerequisites \u00b6 Subscription with Owner permissions Sufficient permissions to AAD (Azure Active Directory) - to create app registrations / service principals Azure DevOps project with Project Administrator permissions Best practises \u00b6 Subscription per environment We consider as a best practice to have separate subscriptions for dev/test/prod environments. Naming of service principals Our recommendation for naming is that the service principal name should reflect purpose and it's permission scope, eg. devops-service-connection-to-{subscription-name} devops-service-connection-to-{subscription}-{resource-group-name} Naming of service connections in DevOps We like to keep name for DevOps service connection same as the name of service principal we are authenticating through. Security Architecture \u00b6 This diagram helps to visualize what is going to be done. It only shows it for dev environment. Notice This tutorial describes how to setup dev environment. Setup for test and prod is identical. 1. Create service principal in Azure Active Directory \u00b6 Go to Azure portal Click on Azure Active Directory Click on App registrations Click on New registration Fill the name and click Register In this case the service principal's purpose is to authorize from DevOps to Azure Cloud and the service principal permission scope is to subscription dslabsdev hence the name devops-service-connection-to-dslabsdev . Copy Application ID and store it for later use Notice Assigning permission scope for service principal will be done in next steps so don't worry if it doesn't make sense yet. 2. Grant service principal permissions to Azure Active Directory API \u00b6 Click on API Permissions Click on Add a permission Click on Microsoft Graph Click on Application permissions Click on Application Select Application.ReadWrite.OwnedBy Click Add permissions You need to repeat this process do grant same permission to legacy API. Reason for this that some Microsoft tools still using this legacy API. az ad app permission add --id <service_principal_id> --api 00000002 -0000-0000-c000-000000000000 --api-permissions 824c81eb-e3f8-4ee6-8f6d-de7f50d565b7 = Role These changes must be approved by some AAD Administrator. Security considerations These permissions only allow to read and manage applications created by service principal. So there is no risk that this service principal can be miused to read or modify any other AAD information. 3. Generate Application secret \u00b6 Click on Certificates & secrets Click on New client secret Add secret description Select expiration - Never Click Add Copy the secret value and store it for later use 4. Grant newly created service principal Owner permissions to your subscription \u00b6 In Azure portal click on Subscriptions You might need to uncheck global subscription filter Click on your dev subscription Click on Access control (IAM) Click on Add Click on Add role assignment Select Owner role Find your devops service principal Click on that service principal Click Save Why Owner role? The pipeline for deploying Resources uses post deploy script to set up secure integration between Databricks, Key Vault, Storage and this can be done only when Service principal has Owner role set. Security considerations Giving service principal permissions on subscription level might be risky. The Owner/Contributor role allow any creation or deletion of resources in that subscription so if you have another projects/resources in that subscription the service principal might be misused to delete them. Assign permissions at subscription level only if the subscription is empty. 5. Create service connection in Azure DevOps \u00b6 Go to any Azure DevOps project Click on settings Click on Service connections Click on New service connection Select Azure Resource Manager Click on Next Select Service principal (manual) Click on Next Fill in approptiate information Service Principal ID = Application ID that was stored in previous steps Service Principal key = Secret that was stored in previous steps You can find Subscription ID in Azure portal under Subscriptions You can find Tenant ID in Azure portal in Active Directory Overview Sevice connection name should be same as service principal name Uncheck Grant access permission to all pipelines Click on Verify and save 6. Create environments \u00b6 Create dev/test/prod environments Set approvals on prod environment 7. Make sure that your subscriptions have appropriate resource providers registered \u00b6 Explicit list , you can use it to filter the Resources: Microsoft.Notebooks Microsoft.ManagedIdentity Microsoft.ContainerRegistry Microsoft.ContainerInstance Microsoft.MachineLearningServices microsoft.insights Microsoft.PolicyInsights Microsoft.Storage Microsoft.Network Microsoft.DataFactory Microsoft.OperationalInsights Microsoft.Databricks Microsoft.KeyVault Microsoft.Security Microsoft.Compute Microsoft.ChangeAnalysis Microsoft.Advisor Microsoft.Commerce Microsoft.ClassicStorage Microsoft.MachineLearning Microsoft.ADHybridHealthService Microsoft.ResourceGraph Microsoft.Resources Microsoft.SerialConsole","title":"Environment setup"},{"location":"chaining-notebook-functions/","text":"Chaining decorated functions \u00b6 Calls of the decorated functions can be chained by passing function names as decorator arguments: @dp . transformation ( dp . read_table ( \"bronze.tbl_customers\" )) def tbl_customers ( df : DataFrame ): return df @dp . transformation ( tbl_customers ) def active_customers_only ( df : DataFrame ): return df . filter ( f . col ( \"active\" ) == 1 ) @dp . transformation ( active_customers_only , display = True ) def save ( df : DataFrame ): return df More compact way: @dp . transformation ( dp . read_table ( \"bronze.tbl_customers\" ), display = True ) def tbl_active_customers ( df : DataFrame ): return df . filter ( f . col ( \"active\" ) == 1 ) Once you run the active_customers_only function's cell, it gets is automatically called with the dataframe loaded by the customers_table function. Similarly, once you run the save_output function's cell, it gets automatically called with the filtered dataframe returned from the active_customers_only function.","title":"Chaining notebook functions"},{"location":"checking-coding-standards/","text":"Checking coding standards \u00b6 Checking that code you write meet some standards is a very common thing in software development. Unfortunately it's not that easy to perform this task in Databricks notebook environment. And that's why pylint for Databricks was created. Why \u00b6 Better readability - if everyone follows same standards it's easier to read code after someone else Early problem detection - linting tools like pylint can discover issues like missing import or undefined variable pretty easily Productivity - Using linting tools leads to better productivity, maybe it doesn't seem that way at first but in long run after people get used to it, it really does Prerequisites \u00b6 If you have existing project all you need to do is Make sure you have pylint in your development dependencies in pyproject.toml Make sure to use benvy 1.3.1 and higher in install_benvy notebook Import pylint notebook to your tools directory in your daipe project In new daipe projects everything above is already setup How to use it \u00b6 There are two main ways to use pylint User flow \u00b6 As a user you write a normal notebook code. If you want to check your code with pylint, you just go to the tools/pylint notebook and click Run All . And that's it. When the notebook finishes you should see pylint results at the bottom of notebook. You can click on the problematic notebooks/files, and you will be automatically redirected to the exact cell with the problem. You can see that pylint tells me that it can't import FeatureStore from featurestorebundle because it is not installed. So it will also detect missing dependencies in your pyproject.toml . If you forget where the problem was you don't have to go back to pylint notebook. We included hint in the redirect URL where you can see the cell and line number where the problem is. CI/CD flow \u00b6 As always people are not as reliable as computers in performing automated tasks. And that's why you should also enable additional linting check in your CI/CD pipeline. If you are using our centralized Github CI/CD pipelines it is pretty easy to enable linting check. Just enable pylint in your build pipeline like that. jobs : build : uses : daipe-ai/daipe-project-ci-cd/.github/workflows/build.yml@v1 with : pylint_enabled : true","title":"Checking coding standards"},{"location":"clone-demo-project/","text":"Get the Daipe demo project \u00b6 Prerequisites Enable 'Files in Repos' in your Databricks workspace at Settings -> Admin Console -> Workspace Settings Log into your Databricks Workspace Under Repos open your personal folder and press \"Add Repo\" and enter the Url: https://github.com/daipe-ai/daipe-demo-databricks.git","title":"Clone demo project"},{"location":"coding-daipe-way/","text":"Coding the \"Daipe way\" \u00b6 Daipe greatly simplify datalake(house) management : Tools to simplify & automate table creation, updates and migrations. Explicit table schema enforcing for Hive tables, CSVs, ... Decorators to write well-maintainable and self-documented function-based notebooks Rich configuration options to customize naming standards, paths, and basically anything to match your needs Why function based notebooks? Compared to bare notebooks, the function-based approach brings the following advantages : create and publish auto-generated documentation and lineage of notebooks and pipelines write much cleaner notebooks with properly named code blocks (unit)test specific notebook functions with ease use YAML to configure your notebooks for given environment (dev/test/prod/...) utilize pre-configured objects to automate repetitive tasks","title":"Intro"},{"location":"create-daipe-project/","text":"Clone Daipe Project \u00b6 Prerequisites Enable 'Files in Repos' in your Databricks workspace at Settings -> Admin Console -> Workspace Settings Set up a GitHub personal access token In your Databricks workspace at Settings -> User Settings -> Git Integration select GitHub as a provider and use your new token here Under Repos open your personal folder and press \"Add Repo\": Enter your project HTTPS Clone Url and confirm Run notebook at src/daipeproject/app/bootstrap to validate your setup","title":"Clone to Databricks"},{"location":"create-repo-from-template/","text":"Creating project from template \u00b6 Create a new empty repository on GitHub or any other GIT provider: After creating a repo, press Import on the bottom of the page: Fill the URL of any Daipe project templates and confirm: Your Daipe project is ready to be cloned to Databricks:","title":"Create from template"},{"location":"creating-custom-output-decorator/","text":"Creating a custom output decorator \u00b6 We are going to create a @send_to_api decorator which sends data to an API. Inside our project's src/__project_name__/lib we create a file called send_to_api.py We must adhere to this interface. @DecoratedDecorator class send_to_api ( OutputDecorator ): def __init__ ( self , * args , ** kwargs ): # init def process_result ( self , result : DataFrame , container : ContainerInterface ): # the decorator logic The input arguments of the class are completely arbitrary. We are using an url variable to specify where to send the data. The process_result() function has a fixed interface. It recieves the result DataFrame and a container with Daipe services and configuration. All the objects necessary to process the DataFrame can be obtained from the container e. g. Logger. We define a custom method for sending data to an API. def __send_to_api ( self , df ): df_json = df . toPandas () . to_json () conn = http . client . HTTPSConnection ( self . __url ) conn . request ( \"POST\" , \"/\" , df_json , { 'Content-Type' : 'application/json' }) The complete output decorator class can look like this from logging import Logger import http.client from daipecore.decorator.DecoratedDecorator import DecoratedDecorator from daipecore.decorator.OutputDecorator import OutputDecorator from injecta.container.ContainerInterface import ContainerInterface from pyspark.sql import DataFrame @DecoratedDecorator class send_to_api ( OutputDecorator ): def __init__ ( self , url : str ): self . __url = url def process_result ( self , result : DataFrame , container : ContainerInterface ): logger : Logger = container . get ( \"datalakebundle.logger\" ) logger . info ( f \"Sending { result . count () } records to API\" ) self . __send_to_api ( result ) def __send_to_api ( self , df ): df_json = df . toPandas () . to_json () conn = http . client . HTTPSConnection ( self . __url ) conn . request ( \"POST\" , \"/\" , df_json , { 'Content-Type' : 'application/json' }) In our project we simple import the decorator using from __myproject__.lib.send_to_api import send_to_api","title":"Creating a custom output decorator"},{"location":"creating-decorator-function/","text":"Creating custom decorator function \u00b6 First let's create a folder lib inside the root of our project to contain the custom code. We are going to create a table_stream_read decorator to read a table as stream. Let's create a file called table_stream_read . Now we just need to ashere to the interface. The function must be decorated using @input_decorator_function and it must contain a definition of a wrapper function which it returns. Example code: from daipecore.function.input_decorator_function import input_decorator_function from injecta.container.ContainerInterface import ContainerInterface from pyspark.sql import SparkSession from datalakebundle.table.parameters.TableParametersManager import TableParametersManager @input_decorator_function def read_stream_table ( identifier : str ): def wrapper ( container : ContainerInterface ): table_parameters_manager : TableParametersManager = container . get ( TableParametersManager ) table_parameters = table_parameters_manager . get_or_parse ( identifier ) spark : SparkSession = container . get ( SparkSession ) return spark . readStream . format ( \"delta\" ) . table ( table_parameters . full_table_name ) return wrapper Usage \u00b6 import daipe as dp from __myproject__.lib.read_stream_table import read_stream_table @dp . transformation ( read_stream_table ( \"bronze.steaming_events\" )) @dp . table_overwrite ( \"silver.tbl_loans\" ) def save ( df : DataFrame ): return df . filter ( f . col ( \"type\" ) == \"new_loan\" ) . orderBy ( \"LoanDate\" )","title":"Creating a decorator function"},{"location":"customizing-table-defaults/","text":"Customizing table defaults \u00b6 Sometimes your table names may contain additional flags to explicitly emphasize some meta-information about the data stored in that particular table. Imagine that you have the following tables: customer_e.my_table product_p.another_table The e/p suffixes describe the fact that the table contains encrypted or plain data. What if we need to use that information in our code? You may always define the attribute explictly in the tables configuration: parameters : datalakebundle : table : name_template : '{identifier}' tables : customer_e.my_table : encrypted : True product_p.another_table : encrypted : False If you don't want to duplicate the configuration, try using the defaults config option to parse the encrypted/plain flag into the new encrypted boolean table attribute: parameters : datalakebundle : table : name_template : '{identifier}' defaults : encrypted : !expr 'db_identifier[-1:] == \"e\"' For more complex cases, you may also use a custom resolver to create a new table attribute: from box import Box from datalakebundle.table.identifier.ValueResolverInterface import ValueResolverInterface class TargetPathResolver ( ValueResolverInterface ): def __init__ ( self , base_path : str ): self . __base_path = base_path def resolve ( self , raw_table_config : Box ): encrypted_string = 'encrypted' if raw_table_config . encrypted is True else 'plain' return self . __base_path + '/' + raw_table_config . db_identifier + '/' + encrypted_string + '/' + raw_table_config . table_identifier + '.delta' parameters : datalakebundle : table : name_template : '{identifier}' defaults : target_path : resolver_class : 'datalakebundle.test.TargetPathResolver' resolver_arguments : - '%datalake.base_path%'","title":"Setting table defaults"},{"location":"customizing-table-names/","text":"Customizing table names \u00b6 Table naming can be customized to match your company naming conventions. By default, all tables are prefixed with the environment name (dev/test/prod/...): parameters : datalakebundle : table : name_template : '%kernel.environment%_{identifier}' The {identifier} is resolved to the table identifier defined in the datalakebundle.tables configuration (see above). By changing the name_template option, you may add some prefix or suffix to both the database, or the table names: parameters : datalakebundle : table : name_template : '%kernel.environment%_{db_identifier}.tableprefix_{table_identifier}_tablesufix'","title":"Customizing table names"},{"location":"daipe-2-0-release-notes/","text":"Daipe 2.0: Release Notes \u00b6 Enhancements \u00b6 It is no longer necessary to define tables in a local environment, YAML config is optional . Local environment is only necessary for the initial setup of the project It is now possible to use Daipe without Databricks on whatever Spark environment or even without Spark just using Pandas Functions such as dp.read_csv() and dp.read_table() can be used as arguments for decorators. This completely replaces the functionality of @dp.data_frame_loader , see docs . Example: # Imports import daipe as dp # Old Daipe @dp . data_frame_loader ( display = True ) def my_transformation ( spark : SparkSession ): return spark . read . table ( \"my_database.my_table\" ) # New Daipe @dp . transformation ( dp . read_table ( \"my_database.my_table\" ), display = True ) def my_transformation ( df : DataFrame ): return df - Support for DBR 8.x - Decorator @dp.table_overwrite which overwrites all data in a table with the data from a DataFrame, see docs - Decorator @dp.table_append which appends the data from a DataFrame to a table, see docs - Decorator @dp.table_upsert which updates existing data based on primary_key and inserts new data, see docs Schema now allows you to define a primary_key (used for @dp.table_upsert ), partition_by and tbl_properties , see docs Schema will be generated for you if you do not provide it to the @table_* decorators see example: Schema checking output is greatly improved. Schema diff example: Backwards incompatible changes \u00b6 Schema is no longer loaded automatically from the schema.py file in the notebook folder. Now the schema can be defined inside the notebook as well as imported from a separate file, see docs and example: Command console datalake:table:create-missing has been removed , because it is no longer possible to rely on the tables being defined in YAML config Command console datalake:table:delete renamed to console datalake:table:delete-including-data Deprecations \u00b6 Decorator @dp.data_frame_loader has been deprecated Decorator @dp.data_frame_saver has been deprecated","title":"Daipe 2.0"},{"location":"daipe-local-setup/","text":"Setup Daipe locally \u00b6 Prerequisites The following software needs to be installed first: Miniconda package manager IMPORTANT! - To avoid Anaconda's Terms of Service run: conda config channels --remove defaults conda config channels --append conda-forge This sets up a community-driven conda-forge as the only conda repository. Git for Windows or standard Git in Linux ( apt-get install git ) We recommend using the following IDEs: PyCharm Community or Pro with the EnvFile plugin installed Visual Studio Code with the PYTHONPATH setter extension installed Tu run commands, use Git Bash on Windows or standard Terminal on Linux/Mac Clone your Daipe project repo Run ./env-init.sh to initialize your local python virtual environment Activating the Conda environment by running conda activate $PWD /.venv # or use the `ca` alias Run the daipe command to list all available commands","title":"Setup Daipe locally"},{"location":"daipe-project-templates/","text":"Daipe templates \u00b6 Base template - contains everything needed to start a Daipe project https://github.com/daipe-ai/daipe-template-base.git Feature Store template - Base template + Feature Store https://github.com/daipe-ai/daipe-template-feature-store.git","title":"Daipe templates"},{"location":"data-pipelines-workflow/","text":"Data pipelines development workflow \u00b6 Our default workflow uses three environments: DEV , TEST , PROD . The feature branches can be merged to the master branch once Pull Request is approved: When the Pull Request is made, the feature branch is automatically deployed to the TEST environment and the tests are run As soon as the tests are completed successfully, the release manager can approve the Pull Request to merge the new branch to master Merging is done using the \"squash\" strategy (all changes are squashed into a single commit) Environments customization Use the allowed_environments config option in the [PROJECT_ROOT]/pyproject.toml file to set environments.","title":"Overview"},{"location":"databricks-connect-setup/","text":"Databricks Connect setup \u00b6 Although Databricks Connect is NOT required when coding in notebooks , you may find it useful when working with the datalake management commands . In the [rootpackage]/_config/bundles/databricksbundle.yaml project bundle config, add the following configuration: parameters : databricksbundle : databricks_connect : connection : address : 'https://dbc-123.cloud.databricks.com' token : 'abcd123456' cluster_id : '0416-084917-doles835' Storing tokens and other sensitive information in YAML configs is generally not a good idea. Try moving the token to your environment variables and the .env file located in the project root: parameters : databricksbundle : databricks_connect : connection : address : 'https://westeurope.azuredatabricks.net' token : '%env(DBX_TOKEN)%' cluster_id : '0416-084917-doles835' How to test Databricks connection? \u00b6 To test that your local configuration works properly, activate the virtual environment and run: $ console dbx:test-connection --env=dev The environment you want to test connection against can be changed by using the --env option.","title":"Databricks Connect setup"},{"location":"datalake-resources-setup/","text":"Spinning up Data Lake resources \u00b6 After completing the setup steps you will have all the data lake resources running. A resource group containing all resources is created for each environment you define. Go to setup steps \u2193 Overview of project workflow (environments / branches): Environment Branch Databricks Workspace Databricks Code branch DataFactory Resource DataFactory Pipelines Code branch APP_ENV tmp tmp DBX tmp - - - - sandbox sandbox DBX sandbox [feature branch] (optional) - - dev (if Bricskflow coding standards are used) dev dev DBX dev [feature branch] (required) ADF dev [feature branch] dev test test DBX test [feature branch] (auto-deployment with PR) ADF [feature-branch] (auto-creation with PR) [feature branch] (auto-deployment with PR) dev prod master DBX prod master (auto-deployment after tag) ADF prod master prod Infrastructure repository workflow: branch based deployment each branch represents different environment with environment specific variables master branch holds the truth and deploys prod environment resources Daipe project repository workflow: feature branches are deployed to dev environment pull requests to master branch are deployed to test environment the master branch is presented in dev environment and released to prod environment after tagging the release 1. Create repository for infrastructure and import it's code \u00b6 In Azure DevOps click on repositories Click on dropdown menu Click on New repository Name it e.g. infra Uncheck Add a README Click Create Click on Import In Clone URL fill https://github.com/DataSentics/adap-infra-template.git In Username fill aisuite@datasentics.com In Password fill the password we have provided you with Click on Import 2. Set main infrastructure variables \u00b6 The file .cicd/variables/variables.yml holds the main variables that you can use to customize your infrastructure. The files .cicd/variables/variables-{temp/sand/dev/test/prod}.yml hold specific variables for each environment. Replace the general placeholders in .cicd/variables/variables.yml : TENANT_ID - from Azure setup section 5 PROJECT_NAME - !! should be simple lowercase name (max 5 characters) !! DEVOPS_ACCOUNT_NAME - name of your devops organization DEVOPS_PROJECT_NAME - name of your devops project 3. Create environment based branches \u00b6 Create branches based on environments you want to deploy: (this needs to be done for all the environments you're about to deploy) For updating environment specific variables create branch and name it after the environment you want to deploy, update environment specific variables. Non prod environment: checkout newly created branch in the file .cicd/variables/variables.yml update SERVICE_CONNECTION_NAME variable for the environment you're about to deploy. The service connection decides into which subscription the resource group will be created. How to create service connection update environment specific variables in the file .cicd/variables/variables-{environment}.yml update desired environment variables here change ADMIN_OBJECT_ID to object id of user of your choice. This user will have admin access to created keyvault You can find user object id in Active Directory. After the successful deployment based on next steps merge the commits to the master branch! Prod environment: Prod environment is based on the master branch. When you're about to deploy prod resources, updated the prod based variables through the pull request, optional directly in the master branch. 3. Create DevOps pipeline for infrastructure build & deployment \u00b6 In Azure DevOps click on pipelines Click on New pipeline Select Azure Repos Git Select infra repository It will automaticaly locate file azure-pipelines.yml Click on Save Click on run and select created branch with the variables of the environment you'd like to deploy. The environment resource group based on selected branch is deployed to the Subscription. Run the pipeline again with different branch selected if you'd like to deploy another environment. 4. Create Key Vault Secret Scope in Databricks \u00b6 Create the scope \u00b6 When the pipeline is finished you need to create secret scope for Databricks. !! This needs to be done for all environments you deployed {temp/sand/dev/test/prod} !! Go to Databricks workspace Look in the URL There should be something like https://adb-3076017168624144.4.azuredatabricks.net/?o=3076017168624144 Add #secrets/createScope at the end of URL URL now should look like https://adb-3076017168624144.4.azuredatabricks.net/?o=3076017168624144#secrets/createScope Hit enter and you should be redirected to the page below Fill in information Scope Name - unit-kv DNS Name and Resource ID can be found in key vault properties Add service principal to Key-Vault \u00b6 Try to run the newly created cluster. If the DBX cluster fails on start with message Cluster terminated.Reason:Invalid Argument , add AzureDatabricks service principal to KV Access policies Go to the newly created Key vault Click on Access policies Click on Add Access Policy Check Get and List in the Secret permissions field Click on None selected Find and select AzureDatabricks service principal Click on Add 5. Enable reading of PROD storage from DEV Databricks \u00b6 Add Role assignment Go to Access Control (IAM) page within PROD blob storage Go to the Role Assignment panel and click on Add button \u2192 Add Role assignment Find and select Storage Blob Data Reader role, click on the Next button Select the service principal that belongs to the DEV storage OAuth authentication Add PROD OAuth variables to the cluster settings. You can copy variables from PROD Databricks. Replace TENANT_OBJECT_ID placeholder. Replace STORAGE_ACCOUNT_NAME placeholder (e.g. adapczp360lakeg2prod ) fs.azure.account.oauth.provider.type.<STORAGE_ACCOUNT_NAME>.dfs.core.windows.net org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider fs.azure.account.oauth2.client.secret.<STORAGE_ACCOUNT_NAME>.dfs.core.windows.net {{ secrets/unit-kv/dbx-client-secret }} fs.azure.account.oauth2.client.endpoint.<STORAGE_ACCOUNT_NAME>.dfs.core.windows.net https://login.microsoftonline.com/<TENANT_OBJECT_ID>/oauth2/token fs.azure.account.oauth2.client.id.<STORAGE_ACCOUNT_NAME>.dfs.core.windows.net {{ secrets/unit-kv/dbx-client-id }} fs.azure.account.auth.type.<STORAGE_ACCOUNT_NAME>.dfs.core.windows.net OAuth Finally, there should be similar variables for DEV and PROD storage. 6. Resources overview \u00b6 After the infrastructure is deployed you can check the resources under resource group adap-cz-PROJECT_NAME-rg-dev Main components Databricks workspace - this is place where you develop your spark notebooks Storage accoount - this is place where your data lives Key vault - this is place where secrets are stored Data factory - main orchestration engine for your Databricks notebooks Virtual network - Key vault and Databricks clusters are deployed in this virtual network for better isolation","title":"Data Lake resources"},{"location":"datalake-storage-path/","text":"Setting datalake storage path \u00b6 Add the following configuration to config.yaml to set the default storage path for all the datalake tables: parameters : datalakebundle : defaults : target_path : '/mybase/data/{db_identifier}/{table_identifier}.delta' When setting defaults , you can utilize any of the following placeholders: {identifier} - customer.my_table {db_identifier} - customer {table_identifier} - my_table parsed custom fields How to set storage path for a specific table? Storage path of any specific table can be easily changed by adding the target_path attribute to given table's configuration: parameters : datalakebundle : tables : customer.my_table : target_path : '/some_custom_base/{db_identifier}/{table_identifier}.delta'","title":"Setting datalake storage path"},{"location":"datalake-structure/","text":"How does a datalake look like? \u00b6 It is recommended to structure your tables into the following layers: Detailed description bronze - \"staging layer\", raw data from source systems silver - most business logic, one or multiple tables per use-case parsed - data loaded from bronze layer to be stored as datalake tables cleansed - data cleaning, source systems bug fixing, ... Smart Data Model (SDM) - fully prepared data for further analytical and machine-learning use-cases gold - additional filtering/aggregations of silver data (using views or materialized tables) to be served to the final customers. Feature Store - central place where customer/product/... features are stored, managed and governed within the organization. reporting marts - pre-aggregated views of the data suitable for reporting and visualizations.","title":"How does a datalake look like?"},{"location":"decorator-functions/","text":"Decorator functions \u00b6 read_csv \u00b6 dp.read_csv ( path: str, schema: StructType = None, options: dict = None ) Reads a CSV file into a spark DataFrame Parameters: path : str - path to the CSV file schema : StructType, default None - schema of the CSV file options : dict, default None - options passed to spark.read.options(**options) Example: @dp . transformation ( dp . read_csv ( \"/LoanData.csv\" , options = dict ( header = True , inferSchema = True )), display = True ) @dp . table_overwrite ( \"bronze.tbl_loans\" ) def save ( df : DataFrame ): return df . orderBy ( \"LoanDate\" ) read_delta \u00b6 dp.read_delta ( path: str, schema: StructType = None, options: dict = None ) Reads a Delta from a path Parameters: path : str - path to the Delta schema : StructType, default None - Union[str, list], default None - schema of the Delta options : dict, default None - options passed to spark.read.options(**options) read_json \u00b6 dp.read_json ( path: str, schema: StructType = None, options: dict = None ) Reads a json file from a path Parameters: path : str - path to the json file schema : StructType, default None - Union[str, list], default None - schema of the json file options : dict, default None - options passed to spark.read.options(**options) read_parquet \u00b6 dp.read_parquet ( path: str, schema: StructType = None, options: dict = None ) Reads a parquet from a path Parameters: path : str - path to the parquet schema : StructType, default None - Union[str, list], default None - schema of the parquet options : dict, default None - options passed to spark.read.options(**options) read_table \u00b6 dp.read_table ( identifier: str ) Reads a table into a spark DataFrame Parameters: identifier : str - full table name, format db.table_name Example: @dp . transformation ( dp . read_table ( \"silver.tbl_loans\" )) def read_table_bronze_loans_tbl_loans ( df : DataFrame , dbutils : DBUtils ): base_year = dbutils . widgets . get ( \"base_year\" ) return df . filter ( f . col ( \"DefaultDate\" ) >= base_year ) table_params \u00b6 dp.table_params ( identifier: str, param_path_parts: list = None ) Reads parameters from datalakebundle.tables.[ identifier ] Parameters: identifier : str - full table name, format db.table_name param_path_parts : list, default None - Union[str, list], default None - list of parameter levels leading to result","title":"Decorator functions"},{"location":"devops-agents-setup/","text":"Setting up Azure DevOps agents (optional) \u00b6 You need some compute where your CI/CD pipelines are run. You can use Microsoft hosted agents or if you want more control over your CI/CD runtime you can setup self-hosted agents. 1. Create Azure DevOps Agent Pool \u00b6 First you need to create Agent Pool in Azure DevOps. Go to your project in Azure DevOps Click on Settings Click on Agent pools Click on Add pool Select Self-hosted Pool type Fill ADAP Pool in name Uncheck Grant access permission to all pipelines Click Create 2. Create Personal Access Token (PAT) \u00b6 Now we need to create Personal Access Token that will have access to this pool. Click on User setting in upper right corner and then on Personal access tokens Click on New Token In right menu click on Show all scopes Select Agent Pools Read & Manage In Name fill AZP_TOKEN Click Create Copy and store the token for later use 3. Create Container Registry \u00b6 We need registry to store our docker image for agents. In Azure DevOps click on repositories Click on dropdown menu Click on New repository Name it e.g. container-registry Uncheck Add a README Click Create Click on Import In Clone URL fill https://github.com/DataSentics/adap-container-registry-template.git In Username fill your Datasentics email In Password fill your Github password / token Click on Import After import is done open file variables.yml Replace placeholders SERVICE_CONNECTION - your dev service connection, e.g. devops-service-connection-to-devsubscription ACR_NAME - name of the registry, e.g. mydevopsregistry Caution Registry name must be globally unique and may contain only lower case letters up to 23 characters. Create pipeline to deploy your registry In Azure DevOps click on pipelines Click on New pipeline Select Azure Repos Git Select container-registry repository Click Run This pipeline will deploy container registry resource. 4. Create Agents \u00b6 Now we need to deploy the agents it self. Repeat steps from previous section to create new repository but this time name the repository e.g. devops-agents and import code from https://github.com/DataSentics/adap-devops-agents-template.git After import is done open file variables.yml and replace placeholders SERVICE_CONNECTION - your dev service connection, e.g. devops-service-connection-to-devsubscription ACR_NAME - name of your previously created registry ( mydevopsregistry ) DEVOPS_URL - url of your devops organization, e.g. https://dev.azure.com/organization/ You can also set how much agents you want (AGENTS_COUNT) and how much cpu and memory in GB should each agent have (AGENT_CPU, AGENT_MEMORY) Repeat steps from previous section to create pipeline but this time select devops-agents repository Click on Variables Click on New variable In Name fill DEVOPS_PAT In Value fill token from section 2 Check Keep this value secret Click OK Click Save Click Run After few minutes when the pipeline finishes you should be able to see agent (or agents) registered in ADAP Pool 5. Let pipelines know to use this Agent Pool \u00b6 So far we only created Agent Pool and registered agent(s) in that pool. But in order to run jobs/tasks on that pool we must inform our pipelines to use that pool. Our pipelines are defined in repository as a yaml code Lets take look at infra repo created in section Data Lake resources Open file azure-pipelines.yml and you should see following code You can see following code at lines 17 and 18 pool: vmImage: 'ubuntu-20.04' This needs to be replaced for pool: 'ADAP Pool' You also need to make same replacement in file .cicd/templates/release-jobs.yml When You commit the changes next time you run infra pipeline it will run on self-hosted agent registered in ADAP Pool","title":"Self-hosted agents (optional)"},{"location":"github-ci-cd-setup/","text":"Setup CI on GitHub \u00b6 In your GitHub project go to Settings > Secrets > New Repository Secret Create DBX_TOKEN and enter an Access Token created in Databricks Workspace at Settings > User Settings > Access Tokens In your code navigate to src/daipeproject/_config/ In your environment configs (dev, test, prod) change the address to match what workspace you are using for each environment","title":"Setup CI/CD on GitHub"},{"location":"input-decorators/","text":"Input decorators \u00b6 These decorators are used to wrap the entire content of a cell. @dp.transformation \u00b6 @dp.transformation ( *objects, display = False, check_duplicate_columns = True ) Used for decorating a function which manipulates with a DataFrame. Runs the decorated function upon declaration. *objects : an arbitrary number of objects passed to the decorated function display : bool, default False - if True the output DataFrame is displayed check_duplicate_columns : bool, default True - if True raises an Exception if there are duplicate columns in the DataFrame Example: import daipe as dp @dp . transformation ( dp . read_table ( \"silver.tbl_loans\" ), dp . read_table ( \"silver.tbl_repayments\" ), display = True ) @dp . table_overwrite ( \"silver.tbl_joined_loans_and_repayments\" , get_joined_schema ()) def join_loans_and_repayments ( df1 : DataFrame , df2 : DataFrame ): return df1 . join ( df2 , \"LoanID\" ) @dp.notebook_function \u00b6 @dp.notebook_function ( *objects ) Used for decorating any other function which is not decorated with the @dp.transformation decorator. Runs the decorated function upon declaration. Parameters: *objects : an arbitrary number of objects passed to the decorated function display : bool, default False - if True the output DataFrame is displayed check_duplicate_columns : bool, default True - if True raises an Exception if there are duplicate columns in the DataFrame Example: @dp . notebook_function () def download_data (): opener = urllib . request . URLopener () opener . addheader ( \"User-Agent\" , \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\" , ) opener . retrieve ( \"https://www.bondora.com/marketing/media/LoanData.zip\" , \"/loanData.zip\" ) opener . retrieve ( \"https://www.bondora.com/marketing/media/RepaymentsData.zip\" , \"/repaymentsData.zip\" ) Objects available in @dp.transformation and @dp.notebook_function \u00b6 spark: SparkSession dbutils: DBUtils logger: Logger Using Spark and Logger from logging import Logger from pyspark.sql.session import SparkSession @dp . notebook_function () def customers_table ( spark : SparkSession , logger : Logger ): logger . info ( 'Reading my_crm.customers' ) return spark . read . table ( 'my_crm.customers' ) Using DBUtils from pyspark.dbutils import DBUtils @dp . notebook_function () def create_input_widgets ( dbutils : DBUtils ): dbutils . widgets . dropdown ( \"base_year\" , \"2015\" , list ( map ( str , range ( 2009 , 2022 ))), \"Base year\" )","title":"Input decorators"},{"location":"local-machine-limited-access/","text":"Local machine has limited internet access \u00b6 In this case we assume that local machine has limited internet access access, e.g. can't do pip install package . For this case we have prepared repository with files and scripts that support this situation. We use basic python virtual environment instead of conda, because conda needs to reach internet when creating virtual env. You can get this repository at https://github.com/daipe-ai/offline-access.git and copy following files/directories to your daipe project. dependencies/ - directory where all the dependencies (.whl files) are stored so we don't have to install them from PyPI .poetry/ - directory where poetry package manager is stored so we don't have to install it from internet env-init-offline.sh - offline environment initialization script activate.sh - environment activation script deactivate.sh - environment deactivation script azure-pipelines.yml - simple offline devops pipeline You can now do ./env-init-offline.sh which will initialize your local environment without touching the internet. You can activate/deactivate environment using following commands source activate.sh source deactivate.sh After activating virtual environment you should be able to run standard Daipe commands like console dbx:deploy . Note In dependencies directory we included dependencies for Windows/Linux and Python 3.7. So we are able to develop Daipe project on local Windows machine and also use it on some ci/cd Linux agent. Also very important thing is that our target Databricks runtime is DBR 7.3 which is Linux with Python 3.7. If you want to make some changes, e.g. add some python package it's your responsibility to add appropriate wheels in the dependencies/ direcotry. Edge case One edge case we run into in one very strict environment is that we were not able to run console command because it is an executable and only defined set of executables was allowed to run. To avoid this issue we can run console command in this way python .venv/Lib/site-packages/consolebundle/CommandRunner.py dbx:deploy . To make life easier we can add following line in the .bashrc - alias console='python .venv/Lib/site-packages/consolebundle/CommandRunner.py' . Be aware that this way the console command will work only from project root. How to get dependencies \u00b6 Databricks dependencies To get dependencies that are needed to run application on databricks you can use command console dbx:build-dependencies as it was documented in first section of this page. Note Dependencies built with console dbx:build-dependencies are just dependencies that are needed to run application itself excluding development dependencies like flake8 etc. If you also want to build development dependecies you can pass --dev flag. Dependencies built this way should be runable on most linux platforms with appropriate python version that was used in Databricks runtime. Local dependencies To get local dependencies that are specific to your platform you can use this sequence of commands. poetry export --dev --without-hashes -o dev-requirements.txt python -m pip wheel -r dev-requirements.txt -w dependencies/ rm dev-requirements.txt","title":"Local machine access limited"},{"location":"managing-tables-console/","text":"Managing datalake tables using console commands \u00b6 Example : To create the customer.my_table table in your datalake, just type console datalake:table:create customer.my_table --env=dev into your terminal within your activated project's virtual environment. The command connects to your cluster via Databricks Connect and creates the table as configured. Datalake management commands: datalake:table:create [table identifier] [path to TableSchema module] - Creates a metastore table based on it's YAML definition (name, schema, data path, ...) datalake:table:recreate [table identifier] [path to TableSchema module] - Re-creates a metastore table based on it's YAML definition (name, schema, data path, ...) datalake:table:delete-including-data [table identifier] - Deletes a metastore table including data on HDFS datalake:table:optimize [table identifier] - Runs the OPTIMIZE command on a given table","title":"Managing tables using console"},{"location":"offline-databricks-solution/","text":"Databricks clusters don't have internet access \u00b6 This section is maninly for those who are working in some strict environment where you may not have full internet access. However there are couple of assumptions Git for Windows installed Python 3.7 or Conda installed Ability to clone or download Github repository Ability to call Databricks API Databricks Personal Access Token This is one of the most common cases. Daipe framework works on your local machine as usual but Databricks clusters can't reach internet, hence cell %run install_master_package is failing. Daipe framework has built in feature to solve this situation. First you need to build dependencies (python wheel packages) needed by master package. You can check what dependencies are needed by running poetry export --without-hashes . Then you need to add those dependencies (.whl files) to dependencies/ directory at the project root. You can do this process by your self by downloading packages manually or building them on some linux based os/docker. Be aware that if you are downloading the packages manually you need to make sure that You are downloading exact version that is needed You are downloading linux compatible package according to Databricks runtime You are downloading python compatible package according to Databricks runtime python version Or you can use automated way which daipe framework offers. In src/ myproject /_config/bundles/dbxdeploy.yaml you can configure on which Databricks workspace and runtime you want to build packages. parameters : dbxdeploy : target : package : build : databricks : host : '%dbxdeploy.databricks.host%' token : '%dbxdeploy.databricks.token%' job_cluster_definition : spark_version : '7.3.x-scala2.12' node_type_id : 'Standard_DS3_v2' num_workers : 1 Note that to build the dependencies the cluster must have internet access. We assume that if you want to build dependecies with ease you will have some non-productional Databricks workspace with internet access. When your config is setup you can just use command console dbx:build-dependencies and daipe will build dependencies on that cluster for you and automatically downloads them in dependencies/ directory. Now to deploy project in the way that %run install_master_package cell doesn't need to reach internet you need to set offline_install: True in dbxdeploy config. parameters : dbxdeploy : target : package : offline_install : True Finally you can use console dbx:deploy to deploy your project.","title":"No internet access on Databricks"},{"location":"output-decorators/","text":"Output decorators \u00b6 Output decorators are used to persist the output of the decorated function in multiple possible formats - table, delta, csv, json and parquet. @dp.table_overwrite \u00b6 @dp.table_overwrite ( identifier: str, table_schema: dp.TableSchema = None, recreate_table: bool = False, options: dict = None ) Overwrites data in a table with a DataFrame returned by the decorated function Parameters: identifier : str - table name table_schema : dp.TableSchema, default None - TableSchema object which defines fields, primary_key, partition_by and tbl_properties, if None the table is saved with the DataFrame schema recreate_table : bool, default False, if True the table is dropped if exists before written to options : dict, default None - options which are passed to df.write.options(**options) @dp.table_append \u00b6 @dp.table_append ( identifier: str, table_schema: dp.TableSchema = None, options: dict = None ) Appends data to a table with a DataFrame returned by the decorated function Parameters: identifier : str - table name table_schema : dp.TableSchema, default None - TableSchema object which defines fields, primary_key, partition_by and tbl_properties, if None the table is saved with the DataFrame schema options : dict, default None - options which are passed to df.write.options(**options) @dp.table_upsert \u00b6 @dp.table_upsert ( identifier: str, table_schema: dp.TableSchema ) Updates data or inserts new data to a table based on primary key with a DataFrame returned by the decorated function Parameters: identifier : str - table name table_schema : dp.TableSchema, default None - TableSchema object which defines fields, primary_key, partition_by and tbl_properties, if None the table is saved with the DataFrame schema @dp.csv_append \u00b6 @dp.csv_append ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Appends a spark DataFrame to a CSV file Parameters: path : str - path to the CSV file partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @dp.csv_overwrite \u00b6 @dp.csv_overwrite ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Overwrites a CSV file by a spark DataFrame Parameters: path : str - path to the CSV file partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @dp.csv_write_ignore \u00b6 @dp.csv_write_ignore ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Saves a spark DataFrame to a CSV file if it does not exist Parameters: path : str - path to the CSV file partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @dp.csv_write_errorifexists \u00b6 @dp.csv_write_errorifexists ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Saves a spark DataFrame to a CSV file, throws an Exception if it already exists Parameters: path : str - path to the CSV file partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @dp.delta_append \u00b6 @dp.delta_append ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Appends a spark DataFrame to a Delta Parameters: path : str - path to the Delta partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @dp.delta_overwrite \u00b6 @dp.delta_overwrite ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Overwrites a Delta by a spark DataFrame Parameters: path : str - path to the Delta partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @dp.delta_write_ignore \u00b6 @dp.delta_write_ignore ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Saves a spark DataFrame to a Delta if it does not exist Parameters: path : str - path to the Delta partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @dp.delta_write_errorifexists \u00b6 @dp.delta_write_errorifexists ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Saves a spark DataFrame to a Delta, throws an Exception if it already exists Parameters: path : str - path to the Delta partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @dp.json_append \u00b6 @dp.json_append ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Appends a spark DataFrame to a json file Parameters: path : str - path to the json file partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @dp.json_overwrite \u00b6 @dp.json_overwrite ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Overwrites a json file by a spark DataFrame Parameters: path : str - path to the json file partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @dp.json_write_ignore \u00b6 @dp.json_write_ignore ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Saves a spark DataFrame to a json file if it does not exist Parameters: path : str - path to the json file partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @dp.json_write_errorifexists \u00b6 @dp.json_write_errorifexists ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Saves a spark DataFrame to a json file, throws an Exception if it already exists Parameters: path : str - path to the json file partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @dp.parquet_append \u00b6 @dp.parquet_append ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Appends a spark DataFrame to a parquet file Parameters: path : str - path to the parquet file partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @dp.parquet_overwrite \u00b6 @dp.parquet_overwrite ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Overwrites a parquet file by a spark DataFrame Parameters: path : str - path to the parquet file partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @dp.parquet_write_ignore \u00b6 @dp.parquet_write_ignore ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Saves a spark DataFrame to a parquet file if it does not exist Parameters: path : str - path to the parquet file partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options) @dp.parquet_write_errorifexists \u00b6 @dp.parquet_write_errorifexists ( path: str, partition_by: Union[str, list] = None, options: dict = None ) Saves a spark DataFrame to a parquet, throws an Exception if it already exists Parameters: path : str - path to the parquet partition_by : Union[str, list], default None - Union[str, list], default None - one or multiple fields to partition the data by options : dict, default None - options passed to df.write.options(**options)","title":"Output decorators"},{"location":"project-code-structure/","text":"Recommended project code structure \u00b6 For databases and tables in each of bronze/silver/gold layers it is recommended to follow the [db_name/table_name] directory structure: [ PROJECT_ROOT ] /src [ROOT_MODULE_NAME] # most commonly name of your company or product bronze_db_batch tbl_customers.py tbl_products.py tbl_contracts # it is possible to place notebooks in folders with the same name if necessary tbl_contracts.py csv_schema.py ... silver_db_batch tbl_product_profitability.py tbl_customer_profitability.py tbl_customer_onboarding.py ... gold_db_batch vw_product_profitability.py # view on silver_db_batch.tbl_product_profitability tbl_customer_profitability.py # \"materialized\" view on silver_db_batch.tbl_customer_profitability vw_customer_onboarding.py","title":"Project code structure"},{"location":"read-access-prod/","text":"Using production data in dev environment \u00b6 Sometimes it's necessary to have production data on the development environment. For example a Data Scientist needs to build model on a real full dataset, but you don't want to give them write access to the production environment. In this section we will show how it can be accomplished on Azure Databricks and Azure Data Lake Storage gen2, but this principle can be generalized to any Databricks workspace and storage. Prerequisites \u00b6 Available since datalake-bundle=^1.2.1 Development environment \u00b6 Databricks workspace named dbx-ws-dev Storage Account (gen2) named datalakedev Service principal for authentication from Databricks to Storage named the same as the Databricks workspace dbx-ws-dev Production environment \u00b6 Databricks workspace named dbx-ws-prod Storage Account (gen2) named datalakeprod Service principal for authentication from Databricks to Storage named the same as the Databricks workspace dbx-ws-prod 1. Setting permissions \u00b6 First it is necessary to give the dev service principal read access to the prod storage. Go to the datalakeprod resource in Azure portal and select Access Control (IAM) and click on Add Select Storage Blob Data Reader role and click Next Click Select members and select dbx-ws-dev service principal Click Review and Assign 2. Setting spark configuration \u00b6 Now we need to tell spark in the dev workspace what is our production storage and how to authenticate to it. Go to dbx-ws-dev Databricks workspace Create cluster In Spark Config fill this configuration fs.azure.account.auth.type.datalakeprod.dfs.core.windows.net OAuth fs.azure.account.oauth.provider.type.datalakeprod.dfs.core.windows.net org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider fs.azure.account.oauth2.client.endpoint.datalakeprod.dfs.core.windows.net https://login.microsoftonline.com/<tenant_id>/oauth2/token fs.azure.account.oauth2.client.id.datalakeprod.dfs.core.windows.net <client_id> fs.azure.account.oauth2.client.secret.datalakeprod.dfs.core.windows.net <client_secret> Replace datalakeprod with the name of your prod storage account name Replace <tenant_id> with the id of your AAD tenant Replace <client_id> with the application id of your Databricks dev principal dbx-ws-dev Replace <client_secret> with the application secret of your Databricks dev principal dbx-ws-dev The created cluster should now be able to read from production storage. 3. Create tables in hive metastore \u00b6 We need to create production tables in development Databricks hive metastore in order to call dp.read_table . This is a manual one time operation. It's considered a best practice to separate environment at the database level. So we create e.g. a bronze production database CREATE DATABASE IF NOT EXISTS prod_bronze Then we create a production table that points to the production storage CREATE TABLE IF NOT EXISTS prod_bronze . some_table USING DELTA LOCATION 'abfss://container@datalakeprod.dfs.core.windows.net/bronze/some_table.delta' 4. Configure daipe datalake-bundle \u00b6 The last step is to configure Daipe to read from the production storage. Go to src/_config/config.yaml and add a tables section to parameters In this section you can override the database that will be read from This can be done for each table separately parameters : datalakebundle : tables : bronze.some_table : db_name : \"prod_bronze\" You can now read the table using the Daipe decorator function like you are used to import daipe as dp # will result in spark.read.table(\"prod_bronze.some_table\") @dp . transformation ( dp . read_table ( \"bronze.some_table\" )) def read_some_table ( df : DataFrame ): return df If you try to write the same table you have configured to read from production it will fail with a permission error Permissions Make sure you have a correct permissions set, datalake-bundle is not responsible for that in any way. # will result in 403 permission error @dp . transformation ( something ) @dp . table_append ( \"bronze.some_table\" ) def write_some_table ( df : DataFrame ): return df","title":"Read access to production data"},{"location":"setting-table-specific-params/","text":"Setting table-specific configuration \u00b6 Besides the basic configuration options , you can also define configuration for specific datalake tables : parameters : datalakebundle : tables : customer.my_table : params : test_data_path : '/foo/bar' Code of the customer/my_table.py notebook: from logging import Logger import daipe as dp @dp . notebook_function ( dp . table_params ( 'customer.my_table' ) . test_data_path ) def customers_table ( test_data_path : str , logger : Logger ): logger . info ( f 'Test data path: { test_data_path } ' ) The dp.table_params('customer.my_table') function call is a shortcut to using %datalakebundle.tables.\"customer.my_table\".params% string parameter.","title":"Setting table-specific parameters"},{"location":"storage-connection-setup/","text":"Storage connection setup \u00b6 Prerequisites databricks-bundle at version 1.3.0 or higher Summary \u00b6 We are moving from setting up storage connection on each cluster individually to a single setup in a YAML config. Old way \u00b6 Formerly the spark config had to be set up directly on each cluster (in Compute > select cluster > Advanced options): New way \u00b6 Add a config in following format to your config_ env .yaml files (you can also use config variables as usual): parameters : databricksbundle : storages : dev : type : azure_gen2 tenant_id : \"xxxxxxxxx\" storage_name : \"xxxxxxxxxdev\" client_id : secret_scope : unit-kv secret_key : dbx-client-id client_secret : secret_scope : unit-kv secret_key : dbx-client-secret test : type : azure_gen2 tenant_id : \"xxxxxxxxx\" storage_name : \"xxxxxxxxxtest\" client_id : secret_scope : unit-kv secret_key : dbx-client-id client_secret : secret_scope : unit-kv secret_key : dbx-client-secret prod : type : azure_gen2 tenant_id : \"xxxxxxxxx\" storage_name : \"xxxxxxxxxprod\" client_id : secret_scope : unit-kv secret_key : dbx-client-id client_secret : secret_scope : unit-kv secret_key : dbx-client-secret","title":"Storage Connection Setup"},{"location":"table-schema/","text":"TableSchema \u00b6 dp.TableSchema ( fields: list, primary_key: Union[str, list] = None, partition_by: Union[str, list] = None, tbl_properties: dict = None ) Defines a table schema Parameters: fields : list - list of StructField defining columns of the table primary_key : Union[str, list], default None - primary key or a list of keys used for @dp.table_upsert partition_by : Union[str, list], default None - one or multiple fields to partition the data by, optional tbl_properties : dict, default None - key value pairs to be added to TBLPROPERTIES , optional Example: import daipe as dp def get_schema (): return dp . TableSchema ( [ t . StructField ( \"ReportAsOfEOD\" , t . DateType (), True ), t . StructField ( \"LoanID\" , t . StringType (), True ), t . StructField ( \"Date\" , t . DateType (), True ), t . StructField ( \"PrincipalRepayment\" , t . DoubleType (), True ), t . StructField ( \"InterestRepayment\" , t . DoubleType (), True ), t . StructField ( \"LateFeesRepayment\" , t . DoubleType (), True ), ], primary_key = [ \"LoanID\" , \"Date\" ], partition_by = \"Date\" , tbl_properties = { \"delta.enableChangeDataFeed\" = \"true\" , } )","title":"Table schema"},{"location":"using-explicit-schema/","text":"Using explicit table schema \u00b6 Table schema can be easily created using the TableSchema class: import daipe as dp def get_schema (): return dp . TableSchema ( [ t . StructField ( \"ReportAsOfEOD\" , t . DateType (), True ), t . StructField ( \"LoanID\" , t . StringType (), True ), t . StructField ( \"Date\" , t . DateType (), True ), t . StructField ( \"PrincipalRepayment\" , t . DoubleType (), True ), t . StructField ( \"InterestRepayment\" , t . DoubleType (), True ), t . StructField ( \"LateFeesRepayment\" , t . DoubleType (), True ), ], primary_key = [ \"LoanID\" , \"Date\" ], # partition_by = \"Date\" ) For more details see the TableSchema reference . Selecting all fields from the schema before writing them into table: \u00b6 import daipe as dp @dp . transformation ( dp . read_csv ( \"loans.csv\" )) @dp . table_overwrite ( \"bronze.tbl_loans\" , get_schema ()) def save ( df : DataFrame ): return ( df . select ( get_schema () . fieldNames ()) ) Schema autosuggestion \u00b6 When using @table_* decorators without an explicit schema,... import daipe as dp @dp . transformation ( dp . read_csv ( \"/RepaymentsData.csv\" , options = dict ( header = True )), ) @dp . table_overwrite ( \"bronze.tbl_repayments\" ) def load_csv_and_save ( df : DataFrame ): return df ...Daipe raises a warning and generates a schema based on the DataFrame for you. Schema checking \u00b6 When using @table_* decorators with an explicit schema, Daipe checks if the schemas match and raises an Exception if they do not. It also shows a difference between the schemas so you can easily fix the problems.","title":"Using explicit schema"},{"location":"using-notebook-function/","text":"Using the @dp.notebook_function() decorator \u00b6 The @dp.notebook_function() decorator is very simple. It recieves any number of arguments, passes it down to its decorated function and runs the function. You can use it for example on a function which downloads data and you want to log its progress. @dp . notebook_function () def download_data ( logger : Logger ): opener = urllib . request . URLopener () opener . addheader ( \"User-Agent\" , \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\" , ) opener . retrieve ( \"https://www.bondora.com/marketing/media/LoanData.zip\" , \"/loanData.zip\" ) logger . info ( \"Loans data successfully downloaded\" ) opener . retrieve ( \"https://www.bondora.com/marketing/media/RepaymentsData.zip\" , \"/repaymentsData.zip\" ) logger . info ( \"Repayments data successfully downloaded\" ) Technical Reference Check the Technical reference for more details about the @dp.notebook_function and other decorators.","title":"Using @dp.notebook_function"},{"location":"using-transformation/","text":"Using the @dp.transformation() decorator \u00b6 There are two main decorators in the Daipe framework - @dp.transformation() and @dp.notebook_function() . @dp.transformation() understands Spark dataframes better and provides you with extra Spark-related functionality like display and duplicate columns checking. @dp.notebook_function() should be used for functions and procedures which don't manipulate with a DataFrame e. g. downloading data. First, import everything necessary for a Daipe pipeline workflow: import daipe as dp Any decorator can also take functions as parameters: @dp . transformation ( dp . read_csv ( \"/data.csv\" , options = dict ( header = True , inferSchema = True ) ), ) def read_csv ( df : DataFrame ): return df See the list of all functions which can be used. display=True option can be used for displaying the DataFrame. @dp . transformation ( dp . read_table ( \"bronze.tbl_customers\" , options = dict ( header = True , inferSchema = True ) ), display = True ) def read_tbl_customers ( df : DataFrame ): return df For more information see the technical reference . Environments \u00b6 Each table is prefixed with an environment tag ( dev , test , prod ) to separate production data from the developement code and vice versa. The Daipe framework automatically inserts the prefix based on your selected environment therefore the code stays the same across all environments.","title":"Using @dp.transformation"},{"location":"using-widgets/","text":"Using Widgets \u00b6 Widgets are a great tool for parametrizing notebooks in both Databricks and Jupyter Lab. Standardized Daipe widgets support Databricks, Jupyter Lab and CLI. # Import daipe import daipe as dp Create a widget: @dp . notebook_function () def create_input_widgets ( widgets : dp . Widgets ): widgets . add_select ( \"base_year\" , list ( map ( str , range ( 2009 , 2022 ))), \"2015\" , \"Base year\" ) No more global variables Following the best practices it is strongly discouraged to create global variables with widget values. Instead of having a global variable, get the value of a widget inside of each decorated function using get_widget_value : @dp . transformation ( dp . read_table ( \"silver.tbl_loans\" ), dp . get_widget_value ( \"base_year\" ), display = True ) def read_table_bronze_loans_tbl_loans ( df : DataFrame , base_year , logger : Logger ): logger . info ( f \"Using base year: { base_year } \" ) return df . filter ( f . col ( \"DefaultDate\" ) >= base_year )","title":"Using widgets"},{"location":"widgets/","text":"Widgets \u00b6 Daipe widgets are cross-platform meaning the same interface works on both Databricks and Jupyter Lab as well as when running a .py file as a script from CLI . Available since databricks-bundle=^1.1.2 - for usage in Databricks jupyter-bundle=^1.1.1 - for usage in Jupyter Lab daipe-core=^1.1.1 - for usage in CLI Imports \u00b6 import daipe as dp Usage \u00b6 widgets.add_text ( name: str, default_value: str = \"\", label: str = None ) Creates a text widget Parameters: name : str - unique identifier of the widget default_value : str, default \"\" - default value of the widget, optional label : str, default \"\" - label of the widget, optional Example: @dp . notebook_function () def create_text_widget ( widgets : dp . Widgets ): widgets . add_text ( \"text_widget\" , \"Hello\" , \"Test widget\" ) widgets.add_select (self, name: str, choices: list, default_value: str, label: str = None) Creates a select/dropdown widget Parameters: name : str - unique identifier of the widget choices : list - the choices to select default_value : str, default \"\" - default value of the widget, optional label : str, default \"\" - label of the widget, optional Example: @dp . notebook_function () def create_select_widget ( widgets : dp . Widgets ): widgets . add_select ( \"select_widget\" , [ \"option1\" , \"option2\" , \"option3\" ], \"option1\" , \"Test widget\" ) widgets.add_multiselect (self, name: str, choices: list, default_value: str, label: str = None) Creates a multiselect widget Parameters: name : str - unique identifier of the widget choices : list - the choices to select default_value : str, default \"\" - default value of the widget, optional label : str, default \"\" - label of the widget, optional Example: @dp . notebook_function () def create_multiselect_widget ( widgets : dp . Widgets ): widgets . add_multiselect ( \"multiselect_widget\" , [ \"option1\" , \"option2\" , \"option3\" ], \"option1\" , \"Test widget\" ) widgets.get_value (self, name: str) Returns a value from a widget Parameters: name : str - unique identifier of the widget Example: @dp . transformation ( load_df ) def get_widget_value ( df : DataFrame , widgets : dp . Widgets ): value = widgets . get_value ( \"text_widget\" ) # value: \"Hello\" return df . filter ( f . col ( \"col\" ) == value ) widgets.remove (self, name: str) Deletes a widget Parameters: name : str - unique identifier of the widget Example: @dp . notebook_function () def remove_widget ( widgets : dp . Widgets ): widgets . remove ( \"text_widget\" ) widgets.remove_all (self) Deletes all widgets Example: @dp . notebook_function () def remove_widget ( widgets : dp . Widgets ): widgets . remove_all ()","title":"Widgets"},{"location":"working-with-git-local/","text":"Working with GIT \u00b6 Create feature branch and work on it \u00b6 One of the most common cases is that you want to create feature branch out of master and make some changes. First activate conda environment $ ca Now pull latest changes from master branch $ git pull origin master Then create a feature branch $ git checkout -b feature-new-great-stuff Finally upload your feature branch to Databricks workspace using following command $ console dbx:deploy --env = dev Your feature branch will be deployed to the DEV Databricks workspace. You can now code some awesome new features right in Databricks workspace! Commit your work to GIT repository \u00b6 Once you're happy with what you've done in your feature branch you probably want to commit and push your changes to git repository. First download all of your work in Databricks workspace to your local machine using following command $ console dbx:workspace:export --env = dev Now you can commit and push your changes to repository $ git add . $ git commit -m \"Awesome new feature\" $ git push origin feature-new-great-stuff Getting in sync with same with existing feature branch folder in Databricks \u00b6 If you are Data Engineer and Data Scientist provided the Databricks folder for you: Create a feature branch on local with the same name as the folder in databricks. Use console dbx:workspace:export to sync the notebooks to the local commit the changes and push them to git service (GitHub, Devops, ...) Updating The Master Package \u00b6 $ console dbx:deploy-master-package --env = dev You have certainly noticed, that in the local Daipe project there is a way more code than in Databricks, where only notebooks exists. It's mainly some yaml and toml configuration files which are uploaded in master package which is installed in each notebook. So if we want to make change to those files our only option is to edit them locally. Example: Adding or updating a project dependency \u00b6 Adding a dependency \u00b6 Suppose we are developing some notebook in Databricks and now we need some new python package (dependency), e.g. scipy . In order to do that we need to follow a series of steps. Add it to pyproject.toml , build master package again and upload it to Databricks. To add scipy to pyproject.toml we need to run poetry add scipy Now we don't want to do console dbx:deploy because it would overwrite our notebooks in Databricks. Instead we want to only update master package. To do that you can use command console dbx:deploy-master-package Now we run %run install_master_package cell again. Now you should be able to import scipy module Important: The updated project should then be pushed to a central repository so that other team members can pull it and have the same dependencies. The dependencies are installed automatically after running git pull Updating a dependency \u00b6 Let's assume that we want to update a depency, e. g. datalake-bundle . We need to follow a series of steps similar to the previous case. We need to check pyproject.toml that the dependency has the correct version defined e. g. datalake-bundle = \"^1.0\" will only update to versions 1.* but not to 2.* or higher. Then we run poetry update datalake-bundle . DO NOT run poetry update without an argument. It updates all packages which might break the entire project. For now on we follow the steps 3 - 6 from the previous example. We use command console dbx:deploy-master-package Now run %run install_master_package cell again. Now you should be able to use the updated datalake-bundle module Important: The updated project should then be pushed to a central repository so that other team members can pull it and have the same dependencies. The dependencies are installed automatically after running git pull","title":"Working with GIT"},{"location":"working-with-git/","text":"Git workflow in Databricks \u00b6 Warning If you are still using Daipe in Workspace / locally, use this guide instead . Prerequisites Enable 'Files in Repos' in your Databricks workspace at Settings -> Admin Console -> Workspace Settings Set up a GitHub personal access token In your Databricks workspace at Settings -> User Settings -> Git Integration select GitHub as a provider and use your new token here Create feature branch and work on it \u00b6 Open the Git menu (branch name next to the repo name, or right click) and checkout a new feature branch: Commit your work to GIT repository \u00b6 After making changes to your feature branch you can commit the changes in the Git menu: Merging the feature branch with your main branch \u00b6 After you are done with the changes, open a Pull Request into your main branch back in your repository on GitHub: Adding, updating or removing a project dependency \u00b6 To add, update or remove a dependency you need to: Open a dbx_poetry notebook in the root of your project. Run the Cmd 1 to show Widgets. Select the add/update/remove action in the first widget and the desired package in the second. Run the rest of the commands If you rerun %run bootstrap in your notebooks, the new dependencies should be available The updated project should then be pushed to a central repository so that other team members can pull it and have the same dependencies. Example \u00b6 Ran dbx_poetry notebook as described above Expected diff in pyproject.toml on commit Expected diff in poetry.lock on commit","title":"Working with git"},{"location":"writing-function-output/","text":"Writing function output \u00b6 Output decorators are used in conjunction with the @dp.transformation input decorator. They take either a string identifier of the table... import daipe as dp @dp . transformation ( dp . read_csv , display = True ) @dp . table_overwrite ( \"bronze.tbl_customers\" ) def save ( df : DataFrame , logger : Logger ): logger . info ( f \"Saving { df . count () } records\" ) return df . withColumn ( \"Birthdate\" , f . to_date ( f . col ( \"Birthdate\" ), \"yyyy-MM-dd\" )) ...or you also skip Hive and write directly into delta: import daipe as dp @dp . transformation ( dp . read_table ( \"bronze.tbl_customers\" ), dp . read_table ( \"bronze.tbl_contracts\" )) @dp . delta_overwrite ( \"/path/to/dataset.delta\" ) def join_tables ( df1 : DataFrame , df2 : DataFrame ): return df1 . join ( df2 , \"Customer_ID\" ) For more details see the Output decorators reference . Automatic schema \u00b6 When using the string table identifier, the @dp.table_overwrite decorator saves the data using the DataFrame schema. This is useful for prototyping. It is highly recommended to use explicit schema for production pipelines.","title":"Writing function output"},{"location":"feature-store/bad-practices/","text":"Bad practices when writing features \u00b6 PySpark offers many functions and methods for developing complex features out of any data. It does not mean that all of them should be used. Shortlist of forbidden functions \u00b6 pyspark.sql.DataFrame.collect() pyspark.sql.DataFrame.toPandas() pyspark.sql.DataFrame.dropDuplicates() pyspark.sql.DataFrame.union() pyspark.sql.DataFrame.count() pyspark.sql.functions.rank() in certain situations Explanations and alternatives \u00b6 DataFrame.collect() \u00b6 Explanation \u00b6 Spark is built upon the idea of lazy evaluation meaning it doesn\"t calculate until it\"s necessary. collect() is an action which means it triggers the calculation and thus breaks the lazy evaluation sequence. It also brings the whole DataFrame onto the driver which might fill its memory and crash it. Alternative \u00b6 If your code contains collect() it is most definitely not optimal, try to come up with a way, how to get the data which needs to be collected into the DataFrame lazily such as by joining the would be collected values to the rows that need it and using the values from there. DataFrame.toPandas() \u00b6 Explanation \u00b6 toPandas() is very similar to collect() . It forces Spark to calculate and it brings the data to the driver. Alternative \u00b6 If your code uses toPandas() then try to rewriting the Pandas logic into PySpark. For example to perform row wise operations in Spark read this . DataFrame.dropDuplicates() \u00b6 Explanation \u00b6 dropDuplicates() always keeps the first occurrence of a \"unique\" row and drops all subsequent duplicates of it. Therefore its outcome is dependent on the order of rows in a DataFrame . Order of rows depends on partitioning and other frequent operations such as join and cache . Alternative \u00b6 To produce replicable and testable code it is necessary NOT to use dropDuplicates() making the code deterministic. A good alternative can be using structs and groupBy allowing you to control how the remaining rows are selected. DataFrame.union() \u00b6 Explanation \u00b6 union() doesn\"t check if the columns are in the same order. It will just glue two DataFrames of the column size together. Alternative \u00b6 Therefore always use unionByName instead of union . DataFrame.count() \u00b6 Explanation \u00b6 count() is an action which triggers calculation. Alternative \u00b6 To preserve the laziness of Spark, if you need to use the number of rows, just use f.count(f.lit(1)) . f.rank() \u00b6 Explanation \u00b6 f.rank() is a useful Window function. Nevertheless it can lead to some unexpected results. rank assigns the same number to rows with equal values. So if you use it in a combination with filter - df.filter(f.rank().over(window) == 1) it gives you multiple rows per rank == 1 . This leads to frequent usage of dropDuplicates to solve this issue. Alternative \u00b6 This algorithm can be better solved by using the solution of dropDuplicates . Example: \u00b6 In this case the most common city is picked based on the freq column when equal to 1 which can result in multiple rows per one client_id due to f.rank returning 1 for all rows with maximum freq . Bad: window_spec = Window . partitionBy ( client_entity . id_column , client_entity . time_column ) . orderBy ( f . col ( \"freq\" ) . desc ()) df_most_common_city = ( card_transactions . groupBy ( \"client_id\" , \"cardtr_transaction_city\" ) . agg ( f . count ( \"cardtr_transaction_city\" ) . alias ( \"freq\" ), f . sum ( \"cardtr_amount_czk\" ) . alias ( \"transaction_city_volume\" ) ) . withColumn ( \"freq_rank\" , f . rank () . over ( window_spec )) . filter ( \"freq_rank == 1\" ) . dropDuplicates ([ client_entity . id_column ]) . select ( \"client_id\" , \"freq\" , \"transaction_city_volume\" , f . col ( \"cardtr_transaction_city\" ) . alias ( \"most_common_city\" )) ) # Result - the frequency is the maximum => 10, but the rest is based on the order of the rows #+---------+----+-----------------------+----------------+ #|client_id|freq|transaction_city_volume|most_common_city| #+---------+----+-----------------------+----------------+ #| 1| 10| 1000| Brno| #+---------+----+-----------------------+----------------+ The alternative version uses structs and their property of comparison as a tuple meaning (x1, x2, x3) <= (x4, x5, x6) only when (x1 <= x4) & (x2 <= x5) & (x3 <= x6) . This gives us a deterministic result based on inequality of either freq , transaction_city_volume or if both are equal the alphabetical order of most_common_city . Good: df_most_common_city = ( card_transactions . withColumn ( \"freq_struct\" , f . struct ( f . count ( \"cardtr_transaction_city\" ) . alias ( \"freq\" ), f . sum ( \"cardtr_amount_czk\" ) . alias ( \"transaction_city_volume\" ), f . col ( \"cardtr_transaction_city\" ) . alias ( \"most_common_city\" ) )) . groupBy ( \"client_id\" ) . agg ( f . max ( f . col ( \"freq_struct\" ) ) . alias ( \"max_freq_struct\" ) ) . select ( \"client_id\" , \"max_freq_struct.*\" ) ) # Result - the frequency is the maximum => 10 and the volume is maximum # and the city is always the first alphabetically if freq and volume are equal #+---------+----+-----------------------+----------------+ #|client_id|freq|transaction_city_volume|most_common_city| #+---------+----+-----------------------+----------------+ #| 1| 10| 2000| Praha| #+---------+----+-----------------------+----------------+","title":"Bad practices"},{"location":"feature-store/daipe-way-dev/","text":"Developing Features daipe way \u00b6 First you need to initialize the feature decorator for entity that you want, e.g. \"client\". from pyspark.sql import types as t from daipecore.decorator.DecoratedDecorator import DecoratedDecorator from featurestorebundle.entity.Entity import Entity from featurestorebundle.feature.FeaturesStorage import FeaturesStorage from featurestorebundle.notebook.decorator.feature import feature entity = Entity ( name = \"client\" , id_column = \"UserName\" , id_column_type = t . StringType (), time_column = \"run_date\" , time_column_type = t . DateType (), ) if not \"client_feature\" in globals (): # client_feature and features_storage are only initialized once features_storage = FeaturesStorage ( entity ) # features_storage stores DataFrames from all feature notebooks @DecoratedDecorator class client_feature ( feature ): # your custom decorator for a specific entity def __init__ ( self , * args , category = None ): super () . __init__ ( * args , entity = entity , category = category , features_storage = features_storage ) Now you can develop the features same way as you were used to, except you wrap the code in daipe decorators. Note that features defined in client_feature decorator must be mapped to the actual features returned by transformation. from pyspark.sql import functions as f from pyspark.sql import DataFrame import daipe as dp @dp . transformation ( dp . read_table ( \"silver.tbl_loans\" ), display = True ) @client_feature ( ( \"Age\" , \"Client's age\" ), ( \"Gender\" , \"Client's gender\" ), ( \"WorkExperience\" , \"Client's work experience\" ), category = \"personal\" , ) def client_personal_features ( df : DataFrame ): return ( df . select ( \"UserName\" , \"Age\" , \"Gender\" , \"WorkExperience\" ) . groupBy ( \"UserName\" ) . agg ( f . max ( \"Age\" ) . alias ( \"Age\" ), f . first ( \"Gender\" ) . alias ( \"Gender\" ), f . first ( \"WorkExperience\" ) . alias ( \"WorkExperience\" ), ) . withColumn ( \"run_date\" , f . lit ( today )) ) Features can be written to Feature Store using Features Writer. Note that features_storage was initialized in the first step. The write step is here for completeness, but it is not recommended writing features in every notebook, instead use write only in orchestration notebook (see section orchestration ). import daipe as dp @dp . notebook_function () def write_features ( writer : DeltaWriter ): writer . write_latest ( features_storage )","title":"Developing features daipe way"},{"location":"feature-store/dbx-fs-integration/","text":"","title":"Dbx fs integration"},{"location":"feature-store/installation/","text":"Installation \u00b6 Installation is seamless by using poetry in yor daipe project. Just type following command in command line. poetry add feature-store-bundle","title":"Installation"},{"location":"feature-store/old-way-dev/","text":"Developing Features old way \u00b6 This is roughly the standard way how features are written. df = spark . read . table ( \"silver.tbl_loans\" ) personal_features_df = ( df . select ( \"UserName\" , \"Age\" , \"Gender\" , \"WorkExperience\" ) . groupBy ( \"UserName\" ) . agg ( f . max ( \"Age\" ) . alias ( \"Age\" ), f . first ( \"Gender\" ) . alias ( \"Gender\" ), f . first ( \"WorkExperience\" ) . alias ( \"WorkExperience\" ), ) . withColumn ( \"run_date\" , f . lit ( today )) ) personal_features_df . write . format ( \"delta\" ) . save ( \"...\" ) This approach has many disadvantages You have to handle writing the dataframe with features by your self Every developer might write features to different locations Might be slow or running in conflicts when multiple notebooks try to write features in same location None features metadata information No track of what was computed and when Feature store tries to address those problems.","title":"Developing features old way"},{"location":"feature-store/orchestration/","text":"Features orchestration \u00b6 Writing features in every notebook is not recommended, because writing the features is expensive operation (delta merge), also if you try to run those notebooks in parallel its most likely you will get write conflict error. Because of that it's considered as a best practice to create features orchestration notebook and write all features at once. % run ./ app / install_master_package % run ./ client_feature_writer_init import daipe as dp from featurestorebundle.delta.DeltaWriter import DeltaWriter % run ./ features_notebook1 % run ./ features_notebook2 # write all features at once @dp . notebook_function () def write_features ( writer : DeltaWriter ): writer . write_latest ( features_storage )","title":"Features orchestration"},{"location":"feature-store/overview/","text":"Feature Store \u00b6 Advantages \u00b6 1. Centralization \u00b6 Centralized storage for features No more features fractured all around your data lake 2. Always up to date \u00b6 Thanks to centralized storage and clever use of orchestration, features are always up to date and ready for modeling and reporting 3. Data Democratization \u00b6 Metadata are automatically saved to metadata table which helps data democratization Metadata are defined in code along with features therefore code is self documenting 4. Performance \u00b6 If you follow best practices for defining features and use orchestration in proper way you get significant performance gain","title":"Overview"},{"location":"feature-store/reading-features/","text":"Reading Features \u00b6 Accessing Feature Store is pretty straightforward. import daipe as dp from featurestorebundle.feature.FeatureStore import FeatureStore @dp . transformation ( display = True ) def load_feature_store ( feature_store : FeatureStore ): return feature_store . get_latest ( \"client\" )","title":"Reading features"},{"location":"feature-store/templates/","text":"Using templates for metadata extraction \u00b6 To get metadata from features, it is necessary to use templates instead of exact feature names. A template is a feature name containing a section in curly braces e. g. feature_example_{time_window} . The part of the actual column name is then matched by the time_window placeholder e. .g if column_name is feature_example_90d then time_window metadata will be 90d . The matched metadata is saved into the Extra key value column of the metadata table and is also propagated into the description using a description template. Template rules Placeholders will match any string which DOES NOT inlude _ . One template CAN match multiple columns. All columns MUST be matched by EXACLY ONE template. All templates MUST match AT LEAST one column. \"{feature_name}\" IS a valid template and it WILL match EVERY column which doesn't include _ and its ONLY piece of metadata will be feature_name . \"there_is_no_template_here\" IS also a valid template and it WILL match ONLY a column with that exact name and it will NOT have any metadata. Templates matching priority is from top to bottom \u2013> always put more specific templates on top and more general template on the bottom. Imports \u00b6 import daipe as dp from featurestorebundle.time_windows import time_windows as tw Features calculation \u00b6 @dp . transformation ( card_transactions ) @client_feature ( # Feature template ( \"card_tr_location_ {location} _ {channel} _ {agg_fun} _ {time_window} \" , # Description template 'Total {agg_fun} of {location} {channel} withdrawals in the last {time_window} .' ), category = 'card_transaction_country_channel' ) def card_channle_country_features ( wdf : WindowedDataFrame ): def country_agg_features ( _ ) -> List [ tw . WindowedColumn ]: return [ tw . sum_windowed ( f . col ( \"cardtr_country\" ) . isin ( \"CZ\" , \"CZE\" ) . cast ( \"integer\" ), \"card_tr_location_czech_count_ {time_window} \" , ), tw . sum_windowed ( ( ~ f . col ( \"cardtr_country\" ) . isin ( \"CZ\" , \"CZE\" )) . cast ( \"integer\" ), \"card_tr_location_abroad_count_ {time_window} \" , ), tw . sum_windowed ( f . when ( f . col ( \"cardtr_country\" ) . isin ( \"CZ\" , \"CZE\" ), f . col ( \"cardtr_amount_czk\" ), ) . otherwise ( 0 ), \"card_tr_location_czech_volume_ {time_window} \" , ), tw . sum_windowed ( f . when ( ~ f . col ( \"cardtr_country\" ) . isin ( \"CZ\" , \"CZE\" ), f . col ( \"cardtr_amount_czk\" ), ) . otherwise ( 0 ), \"card_tr_location_abroad_volume_ {time_window} \" , ), ] def flag_features ( time_window : str ) -> List [ Column ]: return [ tw . column_windowed ( ( f . col ( f \"card_tr_location_abroad_count_ { time_window } \" ) > 0 ) . cast ( \"integer\" ), f \"card_tr_location_abroad_flag_ { time_window } \" , ) ] return wdf . time_windowed ( country_agg_features , flag_features ) Matched metadata \u00b6 All matched placeholders are shown in the Extra column as key value pairs. Notice that the descriptions correspond with the metadata.","title":"Templates"},{"location":"feature-store/time-windows-technical-reference/","text":"Time windows helper classes and functions \u00b6 WindowedDataFrame \u00b6 WindowedDataFrame ( self, df: DataFrame, entity: Entity, time_column: str, time_windows: List[str] ) -> WindowedDataFrame DataFrame which allows for time_windowed calculations df : DataFrame, input DataFrame entity : Entity, the feature store Entity instance, such as client time_column : str, name of Date or Timestamp Column in df , which is subtracted from the run_date to create the time window interval time_windows : List[str] , list of time windows as a [0-9]+[dhw] , suffixes d = days, h = hours, w = weeks Methods: \u00b6 time_windowed \u00b6 time_windowed ( self, agg_columns_function: Callable[[str], List[WindowedColumn]] = lambda x: list(), non_agg_columns_function: Callable[[str], List[Column]] = lambda x: list(), extra_group_keys: List[str] = [], unnest_structs: bool = False ) -> WindowedDataFrame Returns a new WindowedDataFrame with calculated aggregated and non aggregated columns agg_columns_function: Callable[[str], List[WindowedColumn]] = lambda x: list() : Function which takes time_window: str and returns List[WindowedColumn] non_agg_columns_function: Callable[[str], List[Column]] = lambda x: list() : Function which takes time_window: str and returns List[Column] extra_group_keys: List[str] = [] : By default it groups by entity.primary_key , use extra_group_keys to add more columns to group by unnest_structs: bool = False : if True , all structs will be expanded into regular columns Example: @dp . transformation ( card_transactions ) def card_channel_country_features ( wdf : WindowedDataFrame ): def country_agg_features ( time_window : str ) -> List [ tw . WindowedColumn ]: return [ tw . sum_windowed ( f . col ( \"cardtr_country\" ) . isin ( \"CZ\" , \"CZE\" ) . cast ( \"integer\" ), f \"card_tr_location_czech_count_ { time_window } \" , ), tw . sum_windowed ( ( ~ f . col ( \"cardtr_country\" ) . isin ( \"CZ\" , \"CZE\" )) . cast ( \"integer\" ), f \"card_tr_location_abroad_count_ { time_window } \" , ), tw . sum_windowed ( f . when ( f . col ( \"cardtr_country\" ) . isin ( \"CZ\" , \"CZE\" ), f . col ( \"cardtr_amount_czk\" ), ) . otherwise ( 0 ), f \"card_tr_location_czech_volume_ { time_window } \" , ), tw . sum_windowed ( f . when ( ~ f . col ( \"cardtr_country\" ) . isin ( \"CZ\" , \"CZE\" ), f . col ( \"cardtr_amount_czk\" ), ) . otherwise ( 0 ), f \"card_tr_location_abroad_volume_ { time_window } \" , ), ] def flag_features ( time_window : str ) -> List [ Column ]: return [ tw . column ( ( f . col ( f \"card_tr_location_abroad_count_ { time_window } \" ) > 0 ) . cast ( \"integer\" ), f \"card_tr_location_abroad_flag_ { time_window } \" , ) ] return wdf . time_windowed ( country_agg_features , flag_features ) apply_per_time_windowed \u00b6 apply_per_time_window ( self, function: Callable[[WindowedDataFrame, str], DataFrame] ) -> WindowedDataFrame Apply user defined function per time_windows function: Callable[[WindowedDataFrame, str], WindowedDataFrame] : Function which takes WindowedDataFrame and time_window: str and returns WindowedDataFrame Example: @dp . transformation ( frequencies , display = False ) def frequencies_structs ( wdf : WindowedDataFrame ): def make_structs ( wdf : WindowedDataFrame , time_window : str ): return wdf . withColumn ( f \"values_ { time_window } \" , f . struct ( tw . column ( f . col ( f \"transaction_city_count_ { time_window } \" ), f \"card_tr_location_city_most_common_count_ { time_window } \" , ), tw . column ( f . col ( f \"transaction_city_volume_ { time_window } \" ), f \"card_tr_location_city_most_common_volume_ { time_window } \" , ) ), ) return wdf . apply_per_time_window ( make_structs ) is_time_window \u00b6 is_time_window ( self, time_window: str ) -> Column Returns a boolean Column to indicate if a row in the desired time_window time_window: str : time window as a [0-9]+[dhw] , suffixes d = days, h = hours, w = weeks get_windowed_column_list \u00b6 get_windowed_column_list ( self, column_names: List[str] ) -> List[str] Get windowed column names column_names: List[str] : List of column names with a {time_window} placeholder, such as [\"feature1_{time_window}\", \"feature2_{time_window}\", \"feature3_{time_window}\"] make_windowed \u00b6 make_windowed ( df: DataFrame, entity: Entity, time_column: str ) -> WindowedDataFrame Used for creating a WindowedDataFrame instance. df : DataFrame, input DataFrame entity : Entity, the feature store Entity instance, such as client time_column : str, name of Date or Timestamp Column in df , which is subtracted from the run_date to create the time window interval Note: - time_windows are read from a widget called time_windows Example: @dp . transformation ( make_windowed ( card_transactions , client_entity , \"cardtr_process_date\" , ), display = False , ) def card_transactions_with_time_windows ( windowed_card_transactions : WindowedDataFrame ): return windowed_card_transactions WindowedColumn \u00b6 WindowedColumn Alias for type Callable[[str], Column] column \u00b6 column ( name: str, col: Column ) -> Column Alias for col.alias(name) name : name of the column col : PySpark Column Warning No time window functionality : column is intended to only be used in non aggregated columns function, it does not handle time windows on its own. most_common \u00b6 most_common (name: str, *columns: Column): -> Column Performs a most common element calculation based on the input columns name : name of the column columns : PySpark Column Example: This example code calculates most common cities based on the number of transactions conducted in the city. The order of the argument columns determines the outcome. The most common columns is the maximum struct. The maximum struct si determined the same way the max function works in Python. a = ( 10 , - 5 , \"Praha\" ) b = ( 11 , - 1000 , \"Zl\u00edn\" ) c = ( 10 , - 5 , \"Brno\" ) max ( a , b , c ) # Result: (11, -1000, 'Zl\u00edn') max ( a , c ) # Result: (10, -5, \"Praha\") When the number of transactions is 0 for all cities, the result is NULL . @dp . transformation ( city_amount , display = False ) def most_common_city ( wdf : WindowedDataFrame ): def most_common_features ( time_window : str ): return [ tw . most_common ( f \"most_common_city_ { time_window } \" , tw . column ( f \"card_tr_location_city_most_common_count_ { time_window } \" , f . col ( f \"transaction_city_count_ { time_window } \" ) ), tw . column ( f \"random_number_ { time_window } \" , f . hash ( * wdf . primary_key ) ), tw . column ( f \"card_tr_location_city_most_common_ { time_window } \" , f . when ( f . col ( f \"transaction_city_count_ { time_window } \" ) > 0 , f . col ( \"cardtr_transaction_city\" )) ), ) ] return wdf . time_windowed ( most_common_features , unnest_structs = True ) sum_windowed \u00b6 sum_windowed ( name: str, col: Column, default_value=None ) -> WindowedColumn Returns and aggregated WindowedColumn for the PySpark function f.sum name : name of the column with a {time_window} col : PySpark Column default_value = None : value given to all rows not fitting in a current time_window Example: tw . sum_windowed ( \"card_tr_location_czech_count_14d\" , f . col ( \"cardtr_country\" ) . isin ( \"CZ\" , \"CZE\" ) . cast ( \"integer\" ), ) count_windowed \u00b6 count_windowed ( name: str, col: Column, default_value=None ) -> WindowedColumn Returns and aggregated WindowedColumn for the PySpark function f.count name : name of the column col : PySpark Column default_value = None : value given to all rows not fitting in a current time_window count_distinct_windowed \u00b6 count_distinct_windowed ( name: str, cols: List[Column] , default_value=None ) -> WindowedColumn` Returns and aggregated WindowedColumn for the PySpark function f.countDistinct name : name of the column cols : PySpark Column default_value = None : value given to all rows not fitting in a current time_window min_windowed \u00b6 min_windowed ( name: str, col: Column, default_value=None ) -> WindowedColumn Returns and aggregated WindowedColumn for the PySpark function f.min name : name of the column col : PySpark Column default_value = None : value given to all rows not fitting in a current time_window max_windowed \u00b6 max_windowed ( name: str, col: Column, default_value=None ) -> WindowedColumn Returns and aggregated WindowedColumn for the PySpark function f.max name : name of the column col : PySpark Column default_value = None : value given to all rows not fitting in a current time_window avg_windowed \u00b6 avg_windowed ( name: str, col: Column, default_value=None ) -> WindowedColumn Returns and aggregated WindowedColumn for the PySpark function f.avg name : name of the column col : PySpark Column default_value = None : value given to all rows not fitting in a current time_window mean_windowed \u00b6 mean_windowed ( name: str, col: Column, default_value=None ) -> WindowedColumn Returns and aggregated WindowedColumn for the PySpark function f.mean name : name of the column col : PySpark Column default_value = None : value given to all rows not fitting in a current time_window first_windowed \u00b6 first_windowed ( name: str, col: Column, default_value=None ) -> WindowedColumn Returns and aggregated WindowedColumn for the PySpark function f.first name : name of the column col : PySpark Column default_value = None : value given to all rows not fitting in a current time_window","title":"Time windows"},{"location":"feature-store/time-windows/","text":"Developing features with time windows \u00b6 Before reading this article, it is recommended to get familiar with using templates . If you need to create the same feature just for a multiple time windows e. g. Number of repayments made in the last 30, 60 and 90 days , there is an optimal way to do it. Setup \u00b6 First, helper functions for dealing with time windows need to be imported and variables run_date and time_windows need to be defined. For more information about these functions, see the technical reference . A good practice is to define these using Widgets . import daipe as dp import featurestorebundle.time_windows as tw from featurestorebundle.time_windows.time_windows import WindowedColumn , WindowedDataFrame @dp . notebook_function () def widgets ( widgets : dp . Widgets ): widgets . add_text ( 'time_windows' , \"14d,30d,90d\" , default_value = \"14d,30d,90d\" ) Add time window columns \u00b6 First the input DataFrame containing client_id and run_date is loaded. @dp . transformation ( dp . read_delta ( \" %d atalakebundle.card_transactions.source_path%\" ), display = False , ) def card_transactions ( card_transactions : DataFrame ): return card_transactions The input DataFrame is passed into the make_windowed decorator function which returns WindowedDataFrame instance. WindowedDataFrame inherits from a DataFrame which means it is possible to use all methods and properties as it was a regular DataFrame . It has some added properties such as entity and time_windows which allow it to handle calculations of windowed columns. More about WindowedDataFrame in the technical reference . @dp . transformation ( make_windowed ( card_transactions , client_entity , \"cardtr_process_date\" , ), display = False , ) def card_transactions_with_time_windows ( wdf : WindowedDataFrame ): return wdf Writing time windowed features \u00b6 There are generally two options how to write time windowed features - a declarative and a customizable style. The declarative style is the recommended and should be used in a 95 % of cases. The customizable style should only be used when there is absolutely no way to achieve the same functionality using the declarative style. Option 1: Declarative style \u00b6 Generally, each feature cell consists of aggregating some columns and potentially adding some more non aggregated ones afterwards. Therefore the declarative style has 3 steps: 1. Define a function which takes time_window as argument and returns a list of aggregated Columns 1. Define a function which takes time_window as argument and returns a list of NON aggregated Columns 1. Use an instance of time_windowed method of the WindowedDataFrame instance which handles the proper grouping by [id_column, time_column] and time windowed columns @dp . transformation ( card_transactions , display = False ) @client_feature ( ( 'card_tr_location_ {location} _flag_ {time_window} ' , 'Flag if {location} transactions made in the last {time_window} .' ), ( 'card_tr_location_ {location} _ {agg_fun} _ {time_window} ' , 'Total {agg_fun} of {location} transactions made in the last {time_window} .' ,), category = 'card_transaction_city' , ) def card_country_features ( wdf : WindowedDataFrame ): # 1. Define a function which takes time_window as argument and returns a list of aggregated Columns def country_agg_features ( time_window : str ) -> List [ WindowedColumn ]: return [ tw . sum_windowed ( f . col ( \"cardtr_country\" ) . isin ( \"CZ\" , \"CZE\" ) . cast ( \"integer\" ), f \"card_tr_location_czech_count_ { time_window } \" , ), tw . sum_windowed ( ( ~ f . col ( \"cardtr_country\" ) . isin ( \"CZ\" , \"CZE\" )) . cast ( \"integer\" ), f \"card_tr_location_abroad_count_ { time_window } \" , ), tw . sum_windowed ( f . when ( f . col ( \"cardtr_country\" ) . isin ( \"CZ\" , \"CZE\" ), f . col ( \"cardtr_amount_czk\" ), ) . otherwise ( 0 ), f \"card_tr_location_czech_volume_ { time_window } \" , ), tw . sum_windowed ( f . when ( ~ f . col ( \"cardtr_country\" ) . isin ( \"CZ\" , \"CZE\" ), f . col ( \"cardtr_amount_czk\" ), ) . otherwise ( 0 ), f \"card_tr_location_abroad_volume_ { time_window } \" , ), ] # 2. Define a function which takes time_window as argument and returns a list of NON aggregated Columns def flag_features ( time_window : str ) -> List [ Column ]: return [ tw . column_windowed ( ( f . col ( f \"card_tr_location_abroad_count_ { time_window } \" ) > 0 ) . cast ( \"integer\" ), f \"card_tr_location_abroad_flag_ { time_window } \" , ) ] # 3. Return WindowedDataFrame instance which handles the proper grouping by [id_column, time_column] and time windowed columns return wdf . time_windowed ( country_agg_features , flag_features ) Option 2: Customizable style \u00b6 For some complicated features it might be necessary to use the customizable style. Methods like get_windowed_column_list and apply_per_time_window are provided to simplify some frequently used tasks. If it's necessary to add more non-time-windowed columns into the aggregation, it is possible to get the list using the get_windowed_column_list methods and customize it. For any other operations there is always the apply_per_time_window method which takes as argument a function of two arguments (DataFrame and time_window) and returns a modified DataFrame. @dp . transformation ( df_ct_amount , display = False ) @client_feature ( ( \"card_tr_location_cz_city_count_ {time_window} \" , \"Number of different Czech cities where transaction was made in last {time_window} days.\" , ), ( \"card_tr_location_city_most_common_ {agg_fun} _pctg_ {time_window} \" , \"Percentage of {agg_fun} spent in the most common city in last {time_window} days.\" , ), ( \"card_tr_location_city_most_common_ {agg_fun} _ {time_window} \" , \"Total {agg_fun} spent in the most common city in last {time_window} .\" , ), ( \"card_tr_location_city_most_common_ {time_window} \" , \"The city where largest volume of money was spent in last {time_window} .\" , ), category = \"card_transaction_city\" , ) def card_city_features ( wdf_ct_amount : WindowedDataFrame ): # Define a function which takes WindowedDataFrame and time_window and returns WindowedDataFrame def pctg_features ( df : WindowedDataFrame , time_window : str ) -> WindowedDataFrame : return df . withColumn ( f \"card_tr_location_city_most_common_volume_pctg_ { time_window } \" , f . round ( f . col ( f \"card_tr_location_city_most_common_volume_ { time_window } \" ) / f . col ( f \"card_tr_location_volume_ { time_window } \" ), 2 , ), ) . withColumn ( f \"card_tr_location_city_most_common_count_pctg_ { time_window } \" , f . round ( f . col ( f \"card_tr_location_city_most_common_count_ { time_window } \" ) / f . col ( f \"card_tr_location_count_ { time_window } \" ), 2 , ), ) # Get windowed column names cols_to_drop = wdf_ct_amount . get_windowed_column_list ( [ \"card_tr_location_volume_ {time_window} \" , \"card_tr_location_count_ {time_window} \" , ] ) # Use apply_per_time_window to apply the defined function return wdf_ct_amount . apply_per_time_window ( pctg_features ) . drop ( * cols_to_drop )","title":"Time windows"},{"location":"feature-store/feature-store-20/features-development/","text":"Developing Features daipe way \u00b6 First you need to define an entity, and it's id column , e.g. client with primary key client_id . parameters : featurestorebundle : entities : client : id_column : \"client_id\" id_column_type : \"string\" Then you can get the entity, and its feature decorator in notebook. import daipe as dp entity = dp . fs . get_entity () feature = dp . fs . feature_decorator_factory . create ( entity ) Now you can develop the features same way as you were used to, except you wrap the code in daipe decorators. Note that features defined in feature decorator must be mapped to the actual features returned by transformation. import daipe as dp from pyspark.sql import functions as f from pyspark.sql import DataFrame @dp . transformation ( dp . read_table ( \"silver.tbl_loans\" ), display = True ) @feature ( ( \"Age\" , \"Client's age\" ), ( \"Gender\" , \"Client's gender\" ), ( \"WorkExperience\" , \"Client's work experience\" ), category = \"personal\" , ) def client_personal_features ( df : DataFrame ): return ( df . select ( \"UserName\" , \"Age\" , \"Gender\" , \"WorkExperience\" ) . groupBy ( \"UserName\" ) . agg ( f . max ( \"Age\" ) . alias ( \"Age\" ), f . first ( \"Gender\" ) . alias ( \"Gender\" ), f . first ( \"WorkExperience\" ) . alias ( \"WorkExperience\" ), ) . withColumn ( \"run_date\" , f . lit ( today )) )","title":"Features Development"},{"location":"feature-store/feature-store-20/features-tracking/","text":"Features Tracking \u00b6 Feature store can track what features were computed and when. Let's suppose that we have the following Feature Store. And we want to add following dataframe which contains new feature f3 . The Feature Store can keep track that feature f3 was not computed for previous dates. You can think of that RED NULLS as uncomputed values.","title":"Features Tracking"},{"location":"feature-store/feature-store-20/orchestration/","text":"Features orchestration \u00b6 Writing features in every notebook is not recommended, because writing the features is expensive operation (delta merge), also if you try to run those notebooks in parallel its most likely you will get write conflict error. Feature Store address this problem with its own orchestration of feature notebooks. You can define which feature notebooks to orchestrate in config. parameters : featurestorebundle : orchestration : num_parallel : 8 stages : stage1 : - '%databricksbundle.project_root.repo.path%/src/myproject/feature_store/features_ntb1' - '%databricksbundle.project_root.repo.path%/src/myproject/feature_store/features_ntb2' stage2 : - '%databricksbundle.project_root.repo.path%/src/myproject/feature_store/features_ntb3' This configuration means that you want to first run features_ntb1 and features_ntb2 , write results to the feature store and then run features_ntb3 . So stages are executed sequentially to cover computational dependency use cases. You can use num_parallel to control how many notebooks will run in parallel in each stage. You can now use orchestration notebook to run the orchestration. % run ./ app / bootstrap import daipe as dp @dp . notebook_function () def checkpointing_setup ( spark : SparkSession ): spark . sparkContext . setCheckpointDir ( \"dbfs:/tmp/checkpoints\" ) @dp . notebook_function () def orchestrate ( orchestrator : dp . fs . DatabricksOrchestrator ): orchestrator . orchestrate ()","title":"Features Orchestration"},{"location":"feature-store/feature-store-20/reading-features/","text":"","title":"Reading features"},{"location":"feature-store/feature-store-20/target-store/","text":"Target Store \u00b6 Feature store has an interface for reading from Target Store. Target Store is a simple table of e.g. clients, dates and targets . For example target may be First mortgage payment . So we have set of clients and dates of their first mortgage payment. Unfortunately Feature Store not yet provides an interface for writing to Target Store, so it has to be done \"with bare hands\". Paths to target store tables are specified in config. parameters : featurestorebundle : target : table : name_template : 'targets_{entity}' path_template : '%featurestorebundle.base_path%/targets/{entity}.delta' enum_table : name : 'targets_enum' path : '%featurestorebundle.base_path%/targets/targets_enum.delta' backend : delta_path # available: delta_table, delta_path So you basically need two tables. One table holds targets for given entity e.g. client client_id timestamp target_id 1 2019-01-05 first_mortgage_payment 2 2018-06-14 first_mortgage_payment 3 2017-08-24 first_loan_payment Second table is basically enum of targets target_id description first_mortgage_payment Client's first mortgage payment first_loan_payment Client's first loan payment You need to put those tables in appropriate location given by the configuration. If target store is set you can now compute features for given targets. First use WidgetsFactory class to init widgets. import daipe as dp @dp . notebook_function () def init_widgets ( widgets_factory : dp . fs . WidgetsFactory ): widgets_factory . create () You can now select target you want. And finally when loading input dataframe you need to use function with_timestamps which will make sure you will be working only with clients given by selected target. @dp . transformation ( dp . fs . with_timestamps ( dp . read_table ( 'silver.card_transactions' ), entity , ), ) def card_transactions ( card_transactions_with_targets : DataFrame ): return card_transactions_with_targets","title":"Target Store"},{"location":"feature-store/feature-store-20/writing-features/","text":"Writing features \u00b6","title":"Writing features"},{"location":"settle-dq/","text":"Settle DQ \u00b6 Data users trust their scientists and engineers that they deliver reliable data results.\u200b Settle DQ is a data quality solution that enables Data Users and Data Engineers to understand each other and continuously pursue data quality in a way that is relevant to both groups. Settle DQ is integrated with the rest of the AI Platform, but it can be used on its own. Getting started A guide on how to set up Settle DQ on Azure Developing DQ Expectations A guide on how Settle DQ works and how to use it.","title":"Overview"},{"location":"settle-dq/developing-expectations/","text":"Settle DQ Workflow \u00b6 Working with Settle DQ is a continuous process. The Data User and the Data Engineer settle on DQ Agreements - a short description of condtions that the data should satisfy. The Data Engineer then develops DQ Expectations - declarative definitions that check whether data satisfy the given conditions. Then the Data Engineer creates validation notebooks that validate the data against the expectations (this only needs to be done for new checked data sources). The Data User then sees the validation results in a web app every time new data are processed. When conditions change, Data User and the Data Engineer discuss to update and/or create new DQ Agreements .","title":"Workflow Overview"},{"location":"settle-dq/developing-expectations/develop-store/","text":"Developing and Storing DQ Expectations \u00b6 After the Data Engineer settles with the Data User on DQ Agreements, it's time for them to develop expectations. You can think of this process as if you're using a command-line - you write some code that defines expectations and save them to the store. You only run this code once and then you can throw it away. When you want to update your expectations, you don't change your code, but write a different piece of code that updates the expectations in the store. First, you need to run the Configuration Notebook % run ./ path / to / conf_notebook After that, there is a dq_tool variable available to work with the DQ Tool. To develop expectations, you need a playground - an object that is used to define and try out expectations on top of a given metastore table. my_table = 'my_db.customers' playground = dq_tool . get_playground ( table_name = my_table ) By default the playground executes expectations on a limited number of rows. If you want to increase the limit, pass the row_count_limit parameter to get_playground . If you want to use all rows, pass None as a value. An expectation describes a condition that needs to be met by data in a Hive Metastore table. There's a long list of expectation types available. You can also see all available expectations by running playground.list_available_expectation_types() Each expectation type is a method on the playground object. To define an expectation meaning There are at least 400 rows in the table , you execute the following function: line_count = my_playground . expect_table_row_count_to_be_between ( min_value = 9500 , max_value = 11000 ) line_count The moment you execute the function, the expectation is run on the given metastore table. You can see the result and iterate until you reach the definition you're satisfied with. Expectation store is a database containing your expectation definitions. When you're happy with your expectation, add it to the store: dq_tool . expectation_store . add ( expectation = line_count , table_name = my_table , severity = 'error' , agreement = 'The customer count in a dataset exported to the marketing tool should be between 9500 and 11000.' ) The severity parameter defines how serious damage there will be, should the expectation fail. Use 'error' and 'warning' values. The agreement parameter is a text of the agreement you settled upon with the Data User . It should be clear and concise and it should contain explicit values. There's no automatic sync between the agreement and the rest of the definition, so you need to make sure these are consistent. Continue adding expectations by repeating the steps above. The scope of what you can check using expectations is intentionally limited. If you want do some heavy lifting, like computing aggregations, joining different tables, you should do these in a data pipeline, store the results in a table and add an expectation on top of this table.","title":"Developing and Storing DQ Expectations"},{"location":"settle-dq/developing-expectations/edit/","text":"Editing Expectations \u00b6 After you have developed and stored your expectations, you need to make sure these stay up to date. Business requirements change all the time, Data Quality is a continuous process, rather than a one-time project. Think of this process as if you're using a command-line - you write some code that edits expectations and save them to the store. You only run this code once and then you can throw it away. First, you need to run the Configuration Notebook % run ./ path / to / conf_notebook After that, there is a dq_tool variable available to work with the DQ Tool. First, list expectations that are defined for a given table. my_table = 'my_db.customers' dq_tool . expectation_store . print ( table_name = my_table ) database_name: my_db table_name: customers suite_key: default table_expectations: - 1: expect_table_row_count_to_be_between(min_value=9500, max_value=11000) column_expectations: symbol: - 2: expect_column_values_to_not_be_null(column='symbol', mostly=0.99) - 3: expect_column_value_lengths_to_be_between(column='symbol', max_value=3, min_value=1) Each expectation has its expectation_id a number that is unique for a given table (and suite key). You can get an expectation by this number: line_count = dq_tool . expectation_store . get ( table_name = my_table_name , expectation_id = 1 ) print ( line_count ) expect_table_row_count_to_be_between(min_value=9500, max_value=11000) Let's say you acquired some more customers and need to update the expectation. line_count . kwargs [ 'min_value' ] = 10500 line_count . kwargs [ 'max_value' ] = 12000 Then you can run this expectation using a playground object: playground = dq_tool . get_playground ( table_name = my_table ) playground . run_expectation ( line_count ) When you're satisfied with the new definition, you can save it to the store. line_count . agreement = 'The customer count in a dataset exported to the marketing tool should be between 10500 and 12000.' dq_tool . expectation_store . update ( line_count ) Now your updated expectations are used in data validations. You don't need to make any changes to the validation notebook(s) You can also delete an expectation from the store by its id: dq_tool . expectation_store . remove ( table_name = table_name , expectation_id = 3 )","title":"Editing Expectations"},{"location":"settle-dq/developing-expectations/expressions/","text":"Expectations with Expressions \u00b6 When you want to do something slightly custom, but don't want to write custom expectations. Expressions in expectations are used when you want to check something custom, but the logic is not far from an expectation that is already there. Chosen expectations have their expression versions - these work the same as the ordinary ones, the only difference is that instead of a column name, you provide a column_expression . These expressions must be valid spark SQL expressions. Pretty much anything you can write after SELECT in spark SQL can be used as an expression, see the list of functions in SQL spark . The following expression groups are currently supported: single-column expressions : The expression name contains 'expression' behind 'column'. E.g. expect_column_expression_values_to_be_between is an expression version of expect_column_values_to_be_between . Instead of the column parameter, use column_expression . column-pair expressions : E.g. expect_column_expression_pair_values_to_be_equal is an expression version of expect_column_pair_values_to_be_equal . The parameters are named column_expression_A and column_expression_B multi-column expressions : E.g. expect_multicolumn_expression_values_to_be_unique is an expression version of expect_multicolumn_values_to_be_unique . Instead of the column_list parameter use column_expression_list . Warning: Don't go crazy with expressions. If it's too complicated it should probably be a spark transformation with results written to your datalake. DQ Tool should then be used to only check the results of these transformations, not a place where these transformations are defined. If your data analysts aren't well versed in SQL, or you want to hide the logic behind your expectation and keep it at one place, consider using custom expectations. Examples \u00b6 from dq_tool import DQTool dq_tool = DQTool ( spark = spark ) playground = dq_tool . get_playground ( my_df ) See what expression expectations are available: playground . list_available_expectation_with_expersion_types () A single-column expectation \u00b6 In this example we define an expectation: Difference between open and close is smaller or equal 10 , 95% of the time. The definition is as follows. Note you can use all standard parameters of the standard expectation, like mostly . open_close_diff = playground . expect_column_expression_values_to_be_between ( column_expression = 'abs(open - close)' , max_value = 10 , mostly = 0.95 ) print ( open_close_diff ) { \"result\": { \"element_count\": 2000, \"missing_count\": 0, \"missing_percent\": 0.0, \"unexpected_count\": 8, \"unexpected_percent\": 0.4, \"unexpected_percent_nonmissing\": 0.4, \"partial_unexpected_list\": [ 16.089981079101477, 12.25, 10.209991455078125, 28.8800048828125, 11.029998779296875, 16.3800048828125, 10.019989013671875, 10.8599853515625 ] }, \"exception_info\": null, \"meta\": {}, \"success\": true, \"expectation_config\": { \"kwargs\": { \"column_expression\": \"abs(open - close)\", \"max_value\": 10, \"mostly\": 0.95 }, \"expectation_type\": \"expect_column_expression_values_to_be_between\", \"meta\": {} } } A column-pair expectation \u00b6 Similarly, you can define a column-pair expectation. Here we check if rounded open equals rounded close : open_close_rounded = playground . expect_column_expression_pair_values_to_be_equal ( column_expression_A = 'round(open)' , column_expression_B = 'round(close)' ) print ( open_close_rounded ) { \"result\": { \"element_count\": 2000, \"missing_count\": 0, \"missing_percent\": 0.0, \"unexpected_count\": 649, \"unexpected_percent\": 32.45, \"unexpected_percent_nonmissing\": 32.45, \"partial_unexpected_list\": [ [ 78.0, 75.0 ], [ 81.0, 78.0 ], ... ] }, \"exception_info\": null, \"meta\": {}, \"success\": false, \"expectation_config\": { \"kwargs\": { \"column_expression_A\": \"round(open, 0)\", \"column_expression_B\": \"round(close, 0)\" }, \"expectation_type\": \"expect_column_pair_expressions_to_be_equal\", \"meta\": {} } } A multi-column expectation \u00b6 You can also use expressions in multi-column expectations. Here we check that for one symbol and date there should be only one line. For additional fun, we convert the date to a unix timestamp. symbol_date_unique = playground . expect_multicolumn_expression_values_to_be_unique ( column_expression_list = [ 'symbol' , \"unix_timestamp(date, 'yyyy-MM-dd')\" ] ) print ( symbol_date_unique ) { \"result\": { \"element_count\": 2000, \"missing_count\": 0, \"missing_percent\": 0.0, \"unexpected_count\": 0, \"unexpected_percent\": 0.0, \"unexpected_percent_nonmissing\": 0.0, \"partial_unexpected_list\": [] }, \"exception_info\": null, \"meta\": {}, \"success\": true, \"expectation_config\": { \"kwargs\": { \"column_expression_list\": [ \"symbol\", \"unix_timestamp(date, 'yyyy-MM-dd')\" ] }, \"expectation_type\": \"expect_multicolumn_expression_values_to_be_unique\", \"meta\": {} } }","title":"Expectations With Expressions"},{"location":"settle-dq/developing-expectations/validation/","text":"Validating Your Data \u00b6 After the Data Engineer has added expectations, they need to create validation notebook(s) to validate current data against the defined expectations. The validation notebook should have the following structure. Run the Configuration Notebook % run ./ path / to / conf_notebook Run validation on top of your data: my_table = 'my_db.customers' results = dq_tool . expectation_store . validate_table ( table_name = my_table_name ) results After that, there is a dq_tool variable available to work with the DQ Tool. You'll see the validation result dict in your notebook. The results are written to the Expectation Store database. From there, the Data User can see them using the web application","title":"Validating Your Data"},{"location":"settle-dq/developing-expectations/view-validation-results/","text":"Viewing Validation Results \u00b6 The Data User can see their DQ Agreements in the web app deployed in your infrastructure. They can view the last run of each expectation and see the result details.","title":"Viewing Validation Results"},{"location":"settle-dq/getting-started/","text":"Getting started with Settle DQ \u00b6 The schema above shows the parts of the system as they are used by Settle DQ users. Data Engineer works with a python interface that is packaged as a dq_tool python wheel and installed on a Databricks cluster or notebook. Data User works with a web app in their browser. The common ground is a PostgreSQL Azure database.","title":"Overview"},{"location":"settle-dq/getting-started/azure-setup/","text":"Azure Setup \u00b6 The schema above shows the infrastructure the system operates on. You will need to set up the resources displayed in the Your Azure Cloud rectangle. We recommend creating the resources in the order described in this guide. Prerequisites \u00b6 You will need the following prerequisites to start following this guide. Guest user in DataSentics Azure Cloud \u00b6 You will install Settle DQ code from artifacts that are distributed using DataSentics Azure cloud. DataSentics will invite you as a guest user. For that you will need an email address that can receive emails. We recommend using a technical user, not a real person's email. The access to our Azure will be used for downloading and installing artifacts as a part of the initial setup as well as upgrading to future Settle DQ versions. Follow these steps to create a guest user account: Locate an invitation email in your inbox from invites@microsoft.com Click an Accept Invitation link in the email If this email address doesn't have a Microsoft account yet, it needs to sign up for one. Follow the instructions. You will get another email with a verification link or code. You will be promted to accept permissions for DataSentics to Sign in and Read email address. Click accept. You will get yet another email that welcomes you to Azure Devops Now you will be able to login with this guest account to Subscriber resources in DataSentics Azure cloud. Verify you can log in. Artifact Feed URLs from DataSentics \u00b6 DataSentics will give you the following URLs. You'll use them to download artifacts described in the guide. Python Artifact Feed URL, format https://dev.azure.com/xxx/_packaging?_a=feed&feed=dq_tool%40Local private PyPI URL, format pkgs.dev.azure.com/xxx/_packaging/dq_tool%40Local/pypi/simple/ Universal Packages Artifact Feed URL, format https://dev.azure.com/xxx/_packaging?_a=feed&feed=web_files_zip Web App: Container Registry Credentials from DataSentics \u00b6 For deploying the web app, you will need credentials to Azure Container Registry. From there you will pull a docker image to run in your Azure Web App. You will need: Server URL Username Password Image and tag Azure CLI installed \u00b6 Follow the installation instructions Add the DevOps extension: az extension add --name azure-devops Permissions \u00b6 You will need a subscription in your Azure Cloud where you have permissions to the following Resource providers. The ones that are bold are explicitly needed for the system to be set up and work properly. Microsoft.Databricks Microsoft.DataFactory Microsoft.OperationalInsights Microsoft.KeyVault Microsoft.Storage Microsoft.Network Microsoft.ManagedIdentity Microsoft.Security Microsoft.PolicyInsights Microsoft.GuestConfiguration Microsoft.Compute Microsoft.ContainerService Microsoft.Advisor microsoft.insights Microsoft.MachineLearningServices Microsoft.Purview Microsoft.DBforPostgreSQL Microsoft.Web 1. Resource Group \u00b6 We recommend creating a new resource group for all resources that you will create for Settle DQ. 2. Key Vault \u00b6 You will need an Azure Key Vault to store database connection parameters. If you already have a Key Vault, you can use it. If not create one in your Resource Group. 3. Database \u00b6 The database is the center piece of the DQ Tool. The Python API stores expectation definitions and validations results there. The Web App reads information to display from there. If you already have an Azure PostgreSQL Database server, you can just create a new database there. Connect to the server and create a new database . If you don't have a database server, you'll need to create one. We recommend using Azure Database for PostgreSQL. Follow the quickstart guide . We recommend the following options: Single server PosgreSQL 10 Choose the machine depending on your expected workload, the minimal recommended requirements are 1 vcore, 20GB storage For later steps you will need the following parameters. Store them to your Key Vault . host port database username password 4. Databricks \u00b6 If you're not using Azure Databricks yet, you'll need to create a new Databricks Workspace . If you're already using Azure Databricks and have a workspace, you can just use that. You will need an interactive cluster with Databricks Runtime >= 7.0 to install the wheel to. Your Databricks will need access to Azure Key Vault to retrieve the database credentials. Follow the guide to create an Azure Key Vault-backed secret scope . Call it dbx_scope . Now we'll install dq_tool python wheel to Databricks. The wheel provides a python interface that Data Engineers use to develop and manage data expectations . There are two options how to install the wheel. Option A: Manual Wheel Installation \u00b6 We only recommend using this option when your Databricks workspace doesn't have access to internet. The downside is that you will need to manually download and update the wheel with every new version. Download a wheel from the Pytho Artifact Feed URL you got as a prerequisity Upload the wheel to your Databricks workspace and install the wheel to the given cluster or notebook . Option B: Installation from private PyPI \u00b6 This is an automated way to install the dq_tool package. You don't need to store the wheel anywhere, it will always be installed from DataSentics private PyPI that is a part of an Azure DevOps Artifact Feed. First you need to generate a Personal Access Token (PAT) for your guest user. In the Create a new personal access token dialog you need to provide the following: Organization : settle-dq-ext. If you can't see settle-dq-ext, choose All accessible organizations Scopes: Custom defined In Packaging section check Read Name and Expiration are up to you Save the token and add it to your Key Vault as ds_azure_pat along with the technical user email address ds_azure_email and DataSentics PyPI URL ds_pypi_url you got as a prerequisity. In your notebook, install the library using something like user = dbutils . secrets . get ( scope = \"dbx_scope\" , key = \"ds_azure_email\" ) token = dbutils . secrets . get ( scope = \"dbx_scope\" , key = \"ds_azure_pat\" ) pypi_url = dbutils . secrets . get ( scope = \"dbx_scope\" , key = \"ds_pypi_url\" ) % pip install dq_tool -- extra - index - url = https : // $ user : $ token @ $ pypi_url If you get an error like the one below, it means you don't have permissions for the feed, or your PAT is wrong. WARNING: 401 Error, Credentials not correct for https://pkgs.dev.azure.com/xxx/_packaging/dq_tool%40Local/pypi/simple/dq_tool/ ERROR: Could not find a version that satisfies the requirement dq_tool (from versions: none) ERROR: No matching distribution found for dq_tool Configure private PyPI for a Databricks cluster (Optional) \u00b6 To make installation easier for Databricks users, you can do the following setup. First you have to create a Cluster init script if you don't have one and pass it to the cluster. Add the following code to yor init script: mkdir /root/.pip cat > /root/.pip/pip.conf <<EOF [global] extra-index-url=https://$DS_AZURE_EMAIL:$DS_AZURE_PAT@$DS_PYPI_URL EOF Then configure the cluster environment variables as described below. The variable values will be taken from databricks secrets pointing to KeyVault. DS_AZURE_EMAIL={{secrets/dbx_scope/ds_azure_email}} DS_AZURE_PAT={{secrets/dbx_scope/ds_azure_pat}} DS_PYPI_URL={{secrets/dbx_scope/ds_pypi_url}} After this is done, to install dq_tool to your notebook, you only need to run the basic command below, without having to worry about the extra index url. %pip install dq_tool Verify The Installation \u00b6 Run the following code to verify you can connect to the database. from dq_tool import DQTool dq_tool = DQTool ( spark = spark , db_store_connection = { 'drivername' : 'postgresql' , 'host' : dbutils . secrets . get ( scope = 'dbx_scope' , key = 'host' ), 'port' : dbutils . secrets . get ( scope = 'dbx_scope' , key = 'port' ), 'database' : dbutils . secrets . get ( scope = 'dbx_scope' , key = 'database' ), 'username' : dbutils . secrets . get ( scope = 'dbx_scope' , key = 'username' ), 'password' : dbutils . secrets . get ( scope = 'dbx_scope' , key = 'password' ) } ) The code creates the database schema if it's not there yet. Then you can start using the python interface as described in the Develop Expectations guide . Save the code, you'll use it in the Configuration Notebook 5. Web Application \u00b6 The Data User checks the validation results in a web application. It needs an Azure Web app and a Storage account. add Web App Create Web App \u00b6 Create a Web App in the Azure portal . Use the following options: Select your Subscription and Resource Group WebApp name : your choice Publish : select Docker Conatiner Operating system : Linux Service Plan : Create new SKU size : select B1 at least Click Next : Docker > Now you'll need to fill in the credentials for a private Docker registry, you got from DataSentics. Options : Single Container Image Source : Private Registry Click Review + create Create Storage Account \u00b6 Create a Storage Account in the Azure portal . Use the following options: Storage account name : Use 5-50 just alfanumeric characters Performance : Standard Account kind : StorageV2 (general purpose v2) Replication : RA-GRS Click Review + create Enable Static Website \u00b6 Now open your newly created Storage Account. On the left side in Settings click Static Website . Use the following options: Static website : click to set Enabled Index document name : index.html Click Save Change Access Level \u00b6 Now Change access level for the $web container, so that it can be served as a website. Open your Storage Account, click Overview . Under Essentials click Containers . A table of containers opens. Click on three dots on the right side of the $web container Click Change access level Choose Blob (anonymous read access for blobs only) Click OK Upload Frontend Files to Storage Account \u00b6 Now you need to download the web frontend static files zip archive. Open the link you got as desribed in Prerequisites. Download the zip directly from the website, or click Connect to feed to download using the CLI. Unzip the archive. Use the az CLI to upload the contets to the Storage account: az storage blob upload-batch -d '$web' -s ./build --account-name <name of storage> Set Web App Environment Variables \u00b6 You need to point the web app to the database and to the Storage account with the frontend files. Open your Web App, on the left side under Settings click Configuration . Add the following env variables: DB_HOST DB_PORT DB_NAME DB_USER DB_PASS FE_CONTAINER_ADDRESS: URL of the Storage, format https://xxx.web.core.windows.net/ Add authentication to the web app \u00b6 The easiest option to enable authentication is the Express option. To enable authentication using the Express option, follow these steps: In the Azure portal , search for and select App Services , and then select your app. From the left navigation, select Authentication / Authorization > On . Select Azure Active Directory > Express . If you want to choose an existing app registration instead: Choose Select Existing AD app , then click Azure AD App . Choose an existing app registration and click OK . Select OK to register the App Service app in Azure Active Directory. A new app registration is created. Click Save Congratulations \u00b6 You can find the URL of the web app in the Web App Overview, usually in format https://xxx.azurewebsites.net Now you can continue with Developing Expectations Guide to start using Settle DQ.","title":"Azure Setup"},{"location":"settle-dq/getting-started/configuration-notebook/","text":"Settle DQ Configuration Notebook \u00b6 The configuration notebook isn't needed when you use Daipe. If you're using Daipe, continue to the Daipe Integration Guide . If not, continue reading. The python interface to Settle DQ is called DQ Tool. It is distributed as a python wheel that you install to your Databricks cluster. To use DQ Tool, you need to configure it so that it knows how to connect to the database. We highly recommend storing this configuration in a single notebook and run the notebook whenever you need to work with DQ Tool. We higly recommend storing connection paremeters in a Databricks secrets service, especially the password. If you already have these in Azure Key Vault, create a Azure Key Vault-backed secret scope in Databricks and read the params from there. If you want to keep the parameters just in Databricks, create a Databricks-based secret scope and store the parameters there. The notebook should contain code like shown in this example: from dq_tool import DQTool dq_tool = DQTool ( spark = spark , db_store_connection = { 'drivername' : 'postgresql' , 'host' : dbutils . secrets . get ( scope = 'dbx_scope' , key = 'host' ), 'port' : dbutils . secrets . get ( scope = 'dbx_scope' , key = 'port' ), 'database' : dbutils . secrets . get ( scope = 'dbx_scope' , key = 'database' ), 'username' : dbutils . secrets . get ( scope = 'dbx_scope' , key = 'username' ), 'password' : dbutils . secrets . get ( scope = 'dbx_scope' , key = 'password' ) } )","title":"Create Configuration Notebook"},{"location":"settle-dq/getting-started/daipe-integration/","text":"Daipe Integration \u00b6 If you're already using Daipe and want to start using DQ Tool, you need to set it up, as described below. The DQ Tool configuration and instantiation is handled by Daipe, so you can use a DQ Tool instance in any Daipe-decorated notebook function. Setup \u00b6 The following steps assume you already have a daipe project and want to start working with DQ Tool in that project. 1. Add Artifact Feed URL to pyproject.toml \u00b6 The dq-tool-bundle library is distrubuted through a private Datasentics Python package index (artifact feed). Add the following lines to your pyproject.toml so that poetry knows where to get private python wheels. [[tool.poetry.source]] name = \"datasentics\" url = \"<artifact feed url>\" secondary = true The artifact feed url will be given to you along with your token. 2. Set your artifact feed token in poetry \u00b6 Run the following command in your terminal with conda activated: poetry config http-basic.datasentics __token__ <the token> --local Now poetry has credentials to access the artifact feed. 3. Add dq-tool-bundle as a dependency \u00b6 Run the following command in your terminal with conda activated: poetry add dq-tool-bundle Now you have installed the DQ Tool bundle, the DQ Tool and other necessary dependencies. When you deploy to databricks, these will be available there too. 4. Add database connection string to Databricks secrets / Azure Vault \u00b6 Take credentials to your expectation database and store a connection string to your Databricks secrets, or Azure Vault accessible in Datatabricks secrets. The connection string needs to have the following format: postgresql://username:pasword@host:port/database Note that if you have a $ in your password, you'll need to escape it \\$ in the connection string. Next, store the artifact feed token to your Databricks secrets / Azure Vault. Remember the scope and secret names where you have stored these secrets. You'll need them in the next step. 5. Set up cluster environment variables \u00b6 In your databricks cluster you need to set up the following variables. You can find environment variables under cluster configuration -> Advanced Options -> Spark. Set the following variables: DQ_TOOL_DB_STORE_CONNECTION_STRING={{secrets/your_scope/postgres_store_connection_string}} DATABRICKS_HTTP_BASIC_DATASENTICS_PASSWORD={{secrets/your_scope/artifact_feed_token}} DATABRICKS_HTTP_BASIC_DATASENTICS_USERNAME=__token__ You'll need to restart your cluster so that the changes can take effect. Usage \u00b6 Now your can re-deploy your Daipe project and start using DQ Tool. Define expectations \u00b6 To define a new expectation, run a notebook function like the following. import daipe as dp from datalakebundle.table.parameters.TableParametersManager import TableParametersManager from dq_tool import DQTool @dp . notebook_function () def define_expectations_bronze_covid_tbl ( dq_tool : DQTool , table_parameters_manager : TableParametersManager ): # playground lets you run expectation on top of a table params = table_parameters_manager . get_or_parse ( 'my_db.my_table' ) dq_tool . get_playground ( table_name = params . table_identifier , database_name = params . db_identifier ) # the NEVER column values should be between 0 and 1 never_limits = my_playground . expect_column_values_to_be_between ( column = \"NEVER\" , min_value = 0 , max_value = 1 ) print ( never_limits ) For a list of expectations you can define, see the Great Expectations docs After you have fine-tuned your expectation definition, you can save it within a decorated function like this: dq_tool . expectation_store . add ( expectation = never_limits , database_name = my_database_name , table_name = my_table_name , severity = 'error' , agreement = 'The prediction model needs at least 400 rows to predict something meaningful.' , tags = [ 'Data Science' , 'Basic' ] ) Note that this is a throw-away code that doesn't need to be stored in git. All expectation definitions live in your database. To edit expectations, see the editing guide Validate data \u00b6 In your pipeline, you'll want to validate data in a table using expectations you saved in the previous step. Use the following snippet as an example. import daipe as dp @dp . notebook_function () def validate_table ( dq_tool : DQTool , table_parameters_manager : TableParametersManager ): results = dq_tool . expectation_store . validate_table ( database_name = my_database_name , table_name = my_table_name ) print ( results )","title":"Daipe Integration"}]}